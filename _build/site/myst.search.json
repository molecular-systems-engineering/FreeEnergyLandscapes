{"version":"1","records":[{"hierarchy":{"lvl1":"Free Energy Profiles and Equilibrium Constants"},"type":"lvl1","url":"/bindingfreeenergies","position":0},{"hierarchy":{"lvl1":"Free Energy Profiles and Equilibrium Constants"},"content":"Free energy profiles and PMFs link probabilities to partition functions, allowing computation of equilibrium constants for binding reactions.\nCare must be taken when connecting to standard free energies, as\nequilibrium constants in statistical mechanics depend on the reference concentration and are dimensionless, while experimental \\Delta G^\\circ values refer to c^\\circ=1 M. To avoid confusion, here we refer to the (concentration-based) mass action equilibrium constant using the symbol K_c. For association reactions R+L\\rightleftharpoons RL, the mass action law gives K_c=\\frac{[RL]}{[R][L]},\\qquad \\Delta G^\\circ=-kT\\ln(K_c c^\\circ),\n\n but this holds only in the thermodynamic limit. In finite simulation boxes containing one receptor and one ligand, the equilibrium constant is instead expressed through\nthe ratio of Boltzmann probabilities for bound and unbound states \n\nDe Jong et al., 2011:\\Delta G^\\circ=-kT\\ln\\frac{p(RL)}{p(R+L)}-kT\\ln(c^\\circ/c),\n\n where c is the box concentration, and volume change effects are neglected.\nWriting p(RL)/p(R+L)=Q_{RL}/Q_{R+L}, the corresponding mass action\nequilibrium constant becomes K_c=VQ_{RL}/Q_{R+L}. Roux proposed an\nequivalent form using a delta function to fix the ligand in bulk\n\n\nWoo & Roux, 2005\n\nRoux et al., 2008. Gilson et al. derived\nthe same constant from activities for a rigid ligand,\\Delta G^\\circ=-RT\\ln\\left(\\frac{c^\\circ}{8\\pi^2}\\frac{\\sigma_R\\sigma_L}{\\sigma_{RL}}\\frac{Q_{RL}Q_S}{Q_RQ_L}\\right)+P^\\circ\\Delta\\bar V_{AB},\n\nwhere the 8\\pi^2 term arises from rotational integration, \\sigma_i\nare symmetry numbers, and Q_S the solvent partition function. The\ncontribution coming from volume changes\n\\Delta \\bar V_{LR} = V_{LR} -V_L -V_R is spelled out explicitly, and the infinite dilution identity Q_RQ_L/ Q_S = Q_{R+L} has been used.\nThis formulation underpins the double--decoupling method (DDM), in which the ratio of partition functions is obtained by decoupling the ligand at the binding site and in the bulk. Boresch et al. introduced a minimal set of six restraints (one distance r, two angles \\theta_i, and three dihedral angles \\phi_i) to restore the free energy cost of decoupling. The correction is expressed in terms of the respective equilibrium distance r_0, angles \\theta_{i,0}, and force constants K_r, K_{\\theta_i} and K_{\\phi_i}, as\\Delta G_{\\rm restr}=-kT\\ln\\frac{8\\pi^2V\\sqrt{K_rK_{\\theta_1}K_{\\theta_2}K_{\\phi_1}K_{\\phi_2}K_{\\phi_3}}}{r_0^2\\sin\\theta_{1,0}\\sin\\theta_{2,0}(2\\pi kT)^3}.\n\nThe DDM thus provides a general alchemical framework---encompassing\nconfine-and-release variants---whose rigor relies on the proper\napplication and unbiasing of restraints \n\nGilson et al., 1997\n\nBian et al., 2025.","type":"content","url":"/bindingfreeenergies","position":1},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces"},"type":"lvl1","url":"/fg-and-mlcvs","position":0},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces"},"content":"The rapid convergence of machine learning with enhanced sampling and\nfree-energy methodologies marks a genuine paradigm shift: starting with\nthe automated design of collective variables, machine learning is now\nreshaping how we represent, explore, and interpret molecular free-energy\nlandscapes-blurring the boundaries between sampling, bias\nconstruction, and thermodynamic inference, and signalling a revolution\nin the way atomistic simulations generate physical insight. Building on\nthe theoretical and computational foundations discussed in the previous\nsections, this new generation of approaches leverages data-driven\nrepresentations to couple learning and sampling in closed loops,\nenabling adaptive exploration of high-dimensional configuration spaces.\nHere, we focus on some key aspects relevant to the computation and\ninterpretation of free-energy surfaces. For a broader, more\ncomprehensive perspective on the ongoing integration of machine\nlearning, enhanced sampling, and free energy methods, we refer the\nreader to the extensive overviews by Noé et al. \n\nNoé et al., 2020, Mehdi\net al. \n\nMehdi et al., 2024, and Zhu et al. \n\nZhu et al., 2025.","type":"content","url":"/fg-and-mlcvs","position":1},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"Machine-Learning CVs"},"type":"lvl2","url":"/fg-and-mlcvs#machine-learning-cvs","position":2},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"Machine-Learning CVs"},"content":"Traditionally, collective variables (CVs) have been designed from\nphysical intuition, for example, distances in ion-pair association,\ntorsional angles in peptide isomerization, or bond-order parameters in\ncrystallization. Such handcrafted descriptors have been instrumental in\nmolecular simulations for decades, enabling the projection of complex\ndynamics onto interpretable coordinates (see Section\n\n\nsec:CVs). Yet, they are rarely optimal. In high-dimensional\nsystems, where the relevant slow modes arise from nonlinear couplings of\nmany atomic degrees of freedom, physically inspired CVs may fail to\ndistinguish metastable states or to capture the true kinetic bottlenecks\nof phase-space exploration. This realization has prompted the\ndevelopment of machine-learned collective variables (MLCVs), which\nemploy data-driven algorithms to infer optimal low-dimensional\nrepresentations directly from simulation trajectories \n\nGökdemir & Rydzewski, 2025\n\nDesgranges & Delhommelle, 2025\n\nNeha et al., 2022. The\nfield of MLCVs encompasses a spectrum of methodologies that can be\nbroadly grouped by the learning principle they adopt. Unsupervised\napproaches - including principal component analysis \n\nGarcı́a, 1992\n\nAmadei et al., 1993\n\nHegger et al., 2007\n\nWetzel, 2017,\ndiffusion maps \n\nFerguson et al., 2011\n\nPreto & Clementi, 2014,\nsketch-map \n\nCeriotti et al., 2011\n\nTribello et al., 2012, and autoencoders \n\nChen & Ferguson, 2018\n\nWehmeyer & Noé, 2018\n\nLemke & Peter, 2019-learn low-dimensional\nmanifolds that preserve the variance or geometric structure of the\nhigh-dimensional trajectory data. These methods are effective for\ncapturing dominant structural fluctuations, such as protein\nconformational changes or order--disorder transitions in solids. Still,\nthey do not guarantee the recovery of dynamical slow modes. In contrast,\nvariational and supervised methods explicitly target dynamical\nrelevance. The variational approach to conformational dynamics (VAC)\n\n\nNoé & Nuske, 2013\n\nNuske et al., 2014, defines an optimal reaction\ncoordinate as the mapping that maximizes the time-lagged autocorrelation\nof the projected dynamics---equivalently, the leading eigenfunction of\nthe transfer operator. Implementations such as time-lagged independent\ncomponent analysis (TICA) \n\nPérez-Hernández et al., 2013, VAMPnets \n\nMardt et al., 2018, and state-free reversible VAMPnets (SRV) \n\nChen et al., 2019 learn nonlinear transformations that approximate these slow eigenmodes, often in the form of neural-network embeddings.\nClosely related are information-bottleneck formulations, such as RAVE\n(reweighted autoencoded variational Bayes for enhanced sampling) \n\nRibeiro et al., 2018, SPIB \n\nWang et al., 2019 and\nvariational dynamic encoders (VDE) \n\nHernández et al., 2018, which train\nneural networks to identify minimally complex yet maximally predictive\nlatent variables, thus approximating the committor function (see the\ndedicated box above). Complementary to these are discriminant-based\napproaches that use labeled metastable states: linear discriminant\nanalysis (LDA) and its harmonic variant (HLDA) construct transparent,\ndifferentiable linear CVs that maximize between-state separation\n\n\nMendels et al., 2018\n\nPiccini et al., 2018.\nTheir nonlinear extensions, Deep-LDA and Deep-TDA, replace the\nlinear map with a neural network, yielding smooth, expressive CVs that\nintegrate seamlessly with biased enhanced sampling methods such as US,\nMetaD, VES, and OPES for free-energy reconstruction \n\nBonati et al., 2020\n\nTrizio & Parrinello, 2021. These algorithms have found widespread applications across various fields, including ligand binding \n\nWang et al., 2019, conformational transitions\n\n\nHernández et al., 2018\n\nMardt et al., 2018\n\nChen et al., 2019, self-assembly \n\nJung et al., 2023\n\nBoninsegna et al., 2018, and phase transformations \n\nFinney & Salvalaglio, 2023.\n\nA recent frontier in the construction of MLCVs is the use of graph\nneural networks (GNNs) and geometric deep learning architectures, which\nnatively encode the fundamental symmetries of molecular\nsystems - translation, rotation, and permutation invariance of identical\natoms. By representing atomic environments as graphs with nodes (atoms)\nand edges (interactions), these architectures eliminate the need for\nexplicit handcrafted descriptors such as symmetry functions or\nSteinhardt order parameters \n\nDietrich et al., 2023\n\nZhang et al., 2024. Two\nrecent examples demonstrate the potential of this approach. In Dietrich\net al. \n\nDietrich et al., 2023, graph-based models were trained to approximate\nnucleation order parameters in colloidal and metallic systems. The\nlearned variables reproduced the behaviour of conventional Q_6-based\ncrystallinity measures, but with an order-of-magnitude computational\nspeedup, enabling on-the-fly biasing in umbrella sampling and\nmetadynamics. Similarly, Zhang et al. \n\nZhang et al., 2024 introduced\ndescriptor-free collective variables from geometric graph neural\nnetworks, extending the concept to molecular systems and demonstrating\nhow equivariant layers can learn rotationally consistent embeddings of\natomic environments without predefined order parameters.\n\n\n\nFigure 1:Data-driven (CVs) derived from deep learning.\nA,B. Deep-LDA\n\n\nBonati et al., 2020 combines nonlinear neural-network\ntransformations of physical descriptors with linear discriminant\nanalysis to extract a one-dimensional CV that best separates metastable\nbasins, yielding committor-like behavior (panel B) and interpretable\nfeature rankings. C,D. Graph neural-network (GNN) CVs \n\nDietrich et al., 2023 operate directly on molecular graphs built from atomic coordinates, aggregating local environments through message-passing\nlayers and pooling to produce permutation-invariant, differentiable CVs\ntransferable across system sizes and materials. These CVs can be deployed to carry out biased sampling with adaptive methods and thus compute FES, as seen in panel C, associated with the two-step nucleation\nof a colloidal system.\n\nThe natural complement to MLCVs is their integration with enhanced\nsampling methods (see section \n\nsec:Computing), where\nCVs serve as biasing coordinates to accelerate the simulation of rare\nevents. Machine learning can be used both to discover effective CVs and\nto mitigate bias potential in real time. Iterative schemes such as\nactive enhanced sampling alternate between sampling and learning,\nprogressively refining the CV toward a dynamically optimal\ncoordinate \n\nRibeiro et al., 2018\n\nRay et al., 2023\n\nTrizio et al., 2025.\nSuch approaches are often developed in three main steps: (i) learn a\nlow-dimensional representation of slow dynamics, (ii) deploy a given\nlearnt representation as a biasing coordinate in MetaD/VES/OPES/ABF (see\nSection \n\nsec:Computing), and (iii) periodically\nre-train or fine-tune the model on newly sampled configurations. This\niterative workflow, exemplified by, i.e. RAVE \n\nRibeiro et al., 2018, is\nhighlighted and discussed in detail across recent reviews \n\nNoé et al., 2020\n\nMehdi et al., 2024\n\nZhu et al., 2025  and case studies, where the bias is either optimized variationally \n\nValsson & Parrinello, 2014\n\nBonati et al., 2019  or built adaptively from sampling statistics.","type":"content","url":"/fg-and-mlcvs#machine-learning-cvs","position":3},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"Machine-Learning the Committor"},"type":"lvl2","url":"/fg-and-mlcvs#machine-learning-the-committor","position":4},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"Machine-Learning the Committor"},"content":"The accurate sampling and learning of the committor in high-dimensional\nmolecular systems remains an area of active research, uniting\ndevelopments in enhanced-sampling algorithms, transition-path theory and\nmachine-learning representations to make the “ideal” reaction coordinate\na practical and learnable object. In the variational approach presented\nby Kang et al. \n\nKang et al., 2024 and Trizio et al.\n\n\nTrizio et al., 2025, the committor is represented as a\ndifferentiable model\np_B(\\mathbf{r}) = [1 + e^{-q(\\mathbf{r}|w)}]^{-1}, where\nq(\\mathbf{r}|w) is a neural-network, function of physically motivated\ndescriptors. The parameters of the neural network, w, are optimized by\nmaximizing the consistency between predicted and observed transition\noutcomes. This strategy generalizes the likelihood maximization of\nPeters and Trout \n\nPeters & Trout, 2006, and directly yields a smooth,\ndifferentiable approximation to the committor that can be analyzed,\ndifferentiated, and even symbolically regressed to human-interpretable\nforms. Applying a variational principle allows this problem to be\nreformulated in terms of the Kolmogorov functional:K[q] = \\langle |\\nabla q(\\mathbf{r})|^2 \\rangle\n\n whose minimization\nunder boundary conditions q(\\mathbf{r}_A)=0 and q(\\mathbf{r}_B)=1\nyields the committor function satisfying the Kolmogorov equation for\noverdamped dynamics \n\nTrizio et al., 2025.\n\nThis principle defines the Kolmogorov ensemble, in which configurations\nare sampled with probability\np_K(\\mathbf{r}) \\propto e^{-\\beta [U(\\mathbf{r}) + V_K(\\mathbf{r})]},\nand where the committor-dependent bias\nV_K(\\mathbf{r}) = -\\beta^{-1}\\log|\\nabla q(\\mathbf{r})|^2 stabilizes\nconfigurations belonging to the transition-state region \n\nKang et al., 2024.\nUsing this framework, Trizio et al. demonstrated\nthat the learned approximation of the committor can be used not only to\ncharacterize the transition-state ensemble but also to drive enhanced\nsampling by coupling the Kolmogorov bias with on-the-fly probability\nenhanced sampling (OPES)\n\nKang et al., 2024\n\nTrizio et al., 2025. In this extended formulation, the\npre-activation q(\\mathbf{r}) of the neural network trained to yield\nthe committor serves as a smooth CV that acts as a proxy for the\ncommittor itself and enables the exploration of metastable basins and\ntransition states within a single self-consistent workflow. The\nresulting probability-based enhanced sampling approach, was used to model\nprocesses ranging from protein folding to ligand binding, accurately\nreproducing free-energy surfaces and reactive pathways while retaining\ninterpretability and physical transparency \n\n@ Trizio et al., 2025.","type":"content","url":"/fg-and-mlcvs#machine-learning-the-committor","position":5},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"FESs and sampling with MLCVs: some cautionary tales"},"type":"lvl2","url":"/fg-and-mlcvs#fess-and-sampling-with-mlcvs-some-cautionary-tales","position":6},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"FESs and sampling with MLCVs: some cautionary tales"},"content":"As discussed above, MLCVs hold remarkable potential to complement the\ndefinition and calculation of useful FESs. Projecting onto MLCV spaces,\nhowever, raises subtle issues. As highlighted in Ref. \n\nDietrich & Salvalaglio, 2025, the mapping \\xi(\\mathbf{r}), when obtained through\nan MLCV is not uniquely defined: different neural network trainings with\nidentical architectures and hyperparameters can lead to different\nembeddings, altering the Jacobian matrix\nJ_\\xi = \\partial \\xi/\\partial \\mathbf{r}. Given thatp({\\xi}) = \\frac{1}{Z_{NVT}} \\int_{\\Sigma_{\\xi}} e^{-\\beta U(\\mathbf r)} \\mathrm{vol}(J_\\xi)^{-1} d\\sigma\n\nThe shape of F(\\xi) can vary across training instances even when all\nmodels capture the same metastable states. This non-reproducibility\nproblem is specific to machine-learned CVs and is absent in physically\ndefined variables. An effective solution is the adoption of an\nalternative definition of the FES, common to applications in\ncomputational kinetics, i.e., the gauge-invariant or Geometric FES\n\n\nHartmann & Schütte, 2007\n\nHartmann et al., 2011: F_G({\\xi}) = -kT \\ln p_G({\\xi}), \\quad p_G({\\xi}) = \\int_{\\Sigma_{\\xi}} e^{-\\beta U(\\mathbf r)} d\\sigma,\n\n where \\int_{\\Sigma_\\xi} indicates the integral on the hypersurface\ndefined by the level-set of all the configurations degenerate in \\xi, and d\\sigma the infinitesimal element of such hypersurface.\nThis effectively removes the explicit Jacobian dependence of F(\\xi).\nMoreover, F_G(\\xi) is invariant under any monotonic transformation of the CVs, ensuring that the levels of free energy minima, barriers, and saddle points are consistent across different training runs. This gauge\ninvariance makes F_G the natural framework for comparing FESs obtained from independently trained instances of MLCVs with the same architecture, and it favors comparisons across architectures parameterized with different hyperparameters. Another area where the application of MLCVs requires care is the reproducibility of sampling\nefficiency in biased simulations, since variability in the Jacobian arising from stochastic training and hyperparameter choices can lead to unreproducible biasing forces - including spikes or vanishing\ngradients - across different training instances (see Fig. \n\nfig:MLCVs_and_gradientsb) \n\nDietrich & Salvalaglio, 2025. Building on the concept of Geometric FES discussed above, Ref. \n\nDietrich & Salvalaglio, 2025 proposes gradient\nnormalization as a simple and effective approach to mitigate this issue.","type":"content","url":"/fg-and-mlcvs#fess-and-sampling-with-mlcvs-some-cautionary-tales","position":7},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"Machine-Learning--Enhanced Transition-Path Sampling"},"type":"lvl2","url":"/fg-and-mlcvs#machine-learningenhanced-transition-path-sampling","position":8},{"hierarchy":{"lvl1":"Machine-Learned Collective Variables and Their Associated Free Energy Surfaces","lvl2":"Machine-Learning--Enhanced Transition-Path Sampling"},"content":"Machine learning is reshaping transition-path sampling (TPS) by enabling\nadaptive discovery of reactive trajectories in complex systems. Recent\napproaches merge ML inference with path-sampling algorithms, allowing\nthe committor, transition paths, and even full path ensembles to be\nlearned directly from data. Bolhuis and co-workers developed AIMMD \n\nLazzeri et al., 2023, which trains a surrogate model for adaptive reweighting and importance sampling in trajectory space. Applied to\nchignolin folding, it reproduces free energies, rates, and mechanisms at\na fraction of the cost of conventional TPS by learning the committor\nself-consistently. Jung et al. \n\nJung et al., 2023 introduced an\nautonomous ML-TPS framework that iteratively trains a neural-network\ncommittor from shooting outcomes, refocusing sampling near transition\nstates. Tested on ion association, hydrate nucleation, polymer folding,\nand membrane-protein assembly, it yields accurate rates and\ninterpretable mechanisms without predefined reaction coordinates. Chipot\nand co-workers \n\nChen et al., 2023 implemented the\nvariational principle of transition-path theory using neural networks,\ninspiring later committor-consistent schemes such as PCCANN by Megías et\nal., which minimizes the finite-time-lag committor correlation function\nC_{qq}(\\tau)=\\tfrac12\\langle(q(\\tau)-q(0))^2\\rangle, to learn dominant\ntransition tubes. Iteratively coupling biased MD sampling and committor\nretraining, PCCANN identifies committor-consistent pathways and multiple\ncompeting channels, demonstrated for NANMA isomerization and chignolin\nfolding. Together, these studies mark a shift from fixed-bias sampling\nto data-driven path ensembles, in which machine learning reuses\ntrajectory data, reduces costs, and generalizes across systems---laying\nthe groundwork for future generative or diffusion-based transition-path\ntheories.\n\n{#refs .references .csl-bib-body .hanging-indent entry-spacing=“0”}","type":"content","url":"/fg-and-mlcvs#machine-learningenhanced-transition-path-sampling","position":9},{"hierarchy":{"lvl1":"FESs from experiments: single molecule force spectroscopy"},"type":"lvl1","url":"/computing-jarzynski","position":0},{"hierarchy":{"lvl1":"FESs from experiments: single molecule force spectroscopy"},"content":"Single-molecule force spectroscopy (SMFS) methods---such as AFM and\noptical or magnetic tweezers---enable experimental probing of molecular\nfree-energy landscapes. A molecule is stretched between a surface and a\nprobe applying controlled force or displacement, with the extension s\nserving as the reaction coordinate. The resulting mechanical response\nencodes the potential of mean force F(s), recoverable from\nnonequilibrium work measurements via statistical--mechanical relations.\nA central theoretical foundation of this connection is Jarzynski’s\nequality \n\nJarzynski, 1997, which relates nonequilibrium work to\nequilibrium free-energy differences: e^{-\\beta \\Delta G(t)} =\n\\left\\langle e^{-\\beta W(t)} \\right\\rangle ,\n\n where W(t) is the external work performed along\nthe pulling trajectory, and \\langle \\dots \\rangle denotes an average\nover many repetitions of the process. Remarkably, Eq.\n\n\neq:Jarzynski holds even for transformations driven\narbitrarily far from equilibrium, providing a formal bridge between\ndynamical experiments and equilibrium thermodynamics. Hummer and Szabo \n\nHummer & Szabo, 2005, extended this result to the reconstruction of a full free-energy profile along the molecular extension coordinate s,\nobtaining e^{-\\beta F_0(s)} =\n\\left\\langle\n\\delta[s - s(x(t))]  e^{-\\beta [W(t) - V(s(t),t)]}\n\\right\\rangle ,\n\n where V(s,t) is the time-dependent external\npotential applied during pulling. For instance, in optical-tweezer or AFM experiments, a harmonic trap of stiffness k_s is displaced at constant velocity v, V(s,t)=\\tfrac12 k_s(s-vt)^2.\n\nEquation \n\neq:HummerSzabo enables reconstruction of the\nequilibrium free-energy profile F_0(s) from nonequilibrium pulling\ntrajectories. In the adiabatic limit, the work W equals the reversible\n\\Delta F, but real experiments require averaging over many\ntrajectories to account for stochastic fluctuations. Liphardt et al. \n\nLiphardt et al., 2002 first validated this approach, showing RNA\nhairpin unfolding reproduced equilibrium free energies with sub-kT accuracy. These combined experimental--computational approaches reveal\nintermediates and barriers, extending free-energy surface reconstruction into the experimental domain through nonequilibrium work theorems.","type":"content","url":"/computing-jarzynski","position":1},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces"},"type":"lvl1","url":"/computing-biased-fep","position":0},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces"},"content":"","type":"content","url":"/computing-biased-fep","position":1},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Estimating p(\\xi) from Samples: Ergodicity"},"type":"lvl3","url":"/computing-biased-fep#estimating-pxi-from-samples-ergodicity","position":2},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Estimating p(\\xi) from Samples: Ergodicity"},"content":"Computing an FES from Eq. \n\neq:FES requires estimating the\nmarginal equilibrium distribution p(\\xi), typically estimated from\nmolecular dynamics or Monte Carlo trajectories. This estimate assumes\nergodicity, hence implying that sufficiently long trajectories sample\nphase space according to equilibrium weights, making time averages\nequivalent to ensemble averages. Formally, for an observable\nO(\\mathbf{r}):\\lim_{\\tau \\to \\infty} \\frac{1}{\\tau} \\int_0^\\tau O(\\mathbf{r}(t)) dt = \\int O(\\mathbf{r}) f(\\mathbf{r}) d\\mathbf{r}\n\nwhere f(\\mathbf{r}) = \\frac{e^{-\\beta U(\\mathbf{r})}}{Q_{NVT}} is the\ncanonical distribution, U(\\mathbf{r}) is the potential energy, and\nQ_{NVT} the configurational partition function. In the specific case\nof a collective variable \\xi(\\mathbf{r}), the ergodic hypothesis\nensures that the empirical distribution of \\xi obtained from sampling,\nconverges to equilibrium in the long-time limit:p(\\xi) = \\lim_{\\tau \\to \\infty} \\frac{1}{\\tau} \\int_0^\\tau \\delta\\big(\\xi(\\mathbf{r}(t)) - \\xi \\big) dt = \\frac{1}{Q_{NVT}} \\int e^{-\\beta U(\\mathbf{r})} \\delta\\big(\\xi(\\mathbf{r}) - \\xi\\big) d\\mathbf r .\n\nWhen systems feature metastable states separated by high barriers,\nunbiased simulations on practical timescales often fail to achieve\nergodic sampling, leading to incomplete estimates of p(\\xi); enhanced\nsampling methods address this by accelerating rare transitions, ensuring\nthorough exploration of relevant states, and yielding reliable FESs that\ncapture both thermodynamic stability and kinetic accessibility.","type":"content","url":"/computing-biased-fep#estimating-pxi-from-samples-ergodicity","position":3},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Recovering Ergodicity via Biased Sampling: Foundations"},"type":"lvl3","url":"/computing-biased-fep#recovering-ergodicity-via-biased-sampling-foundations","position":4},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Recovering Ergodicity via Biased Sampling: Foundations"},"content":"Biased enhanced sampling methods aim to efficiently achieve ergodicity\nby introducing an artificial bias potential V(\\xi) that modifies the\nsystem Hamiltonian, H_1 = H_0 + V(\\xi),\n\n to alter the equilibrium\ndistribution p(\\xi) into a biased one p_b(\\xi). Reweighting\ntechniques, often rooted in Zwanzig’s free energy perturbation \n\nZwanzig, 1954, enable the recovery of unbiased distributions.\nBuilding on these principles, methods such as Umbrella Sampling, Blue\nMoon Ensemble, and Metadynamics enable the computation of FESs from\nbiased ensembles; for a comprehensive overview, see Ref. \n\nHénin et al., 2022.","type":"content","url":"/computing-biased-fep#recovering-ergodicity-via-biased-sampling-foundations","position":5},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl4":"Free Energy Perturbation and Thermodynamic Integration[]{#sec:FEP label=“sec:FEP”}","lvl3":"Recovering Ergodicity via Biased Sampling: Foundations"},"type":"lvl4","url":"/computing-biased-fep#free-energy-perturbation-and-thermodynamic-integration","position":6},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl4":"Free Energy Perturbation and Thermodynamic Integration[]{#sec:FEP label=“sec:FEP”}","lvl3":"Recovering Ergodicity via Biased Sampling: Foundations"},"content":"The term Free Energy perturbation (FEP) describes a series of methods\noriginating in the thermodynamic perturbation (TP) one, which evaluates\nthe free-energy change when switching from a reference Hamiltonian H_0\nto a perturbed one H_1 via Zwanzig’s formula \n\nZwanzig, 1954,\\Delta F = -kT\\ln \\left\\langle e^{-\\beta(H_1-H_0)}\\right\\rangle_0,\n\nwhere the subscript 0 is a reminder that the average has to be taken\nusing the unperturbed Hamiltonian H_0 to integrate the equations of\nmotions. The formula is exact for any value of H_1, so the term\n“perturbation” is somewhat misleading. However, the sampling is\nefficient only when configurations sampled under H_0 overlap well with\nthose favored by H_1. Stratifying the jump into neighboring states\n\\xi_i\\to \\xi_{i+1} gives multistep TP,\\Delta F=\\sum_i -kT\\ln\\left\\langle e^{-\\beta[H(\\xi_{i+1})-H(\\xi_i)]}\\right\\rangle_{\\xi_i}.\n\nThe principle of TP is at the core of numerous methods described in the\nfollowing sections.","type":"content","url":"/computing-biased-fep#free-energy-perturbation-and-thermodynamic-integration","position":7},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl4":"Thermodynamic Integration","lvl3":"Recovering Ergodicity via Biased Sampling: Foundations"},"type":"lvl4","url":"/computing-biased-fep#thermodynamic-integration","position":8},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl4":"Thermodynamic Integration","lvl3":"Recovering Ergodicity via Biased Sampling: Foundations"},"content":"Thermodynamic integration (TI) is in effect an application of multistep\nTP in the limiting case of infinitesimally small changes:\\Delta F=\\int_{ s_0}^{ s_1}\\Big\\langle \\frac{\\partial H}{\\partial  \\xi}\\Big\\rangle_{ \\xi=s}d s.\n\nEq. \n\neq:TI can be rigorously derived from Eq.\n\n\neq:multiTP using the first term of the cumulant expansion\n\n\nKubo, 1962\n\n\\ln\\left\\langle e^{\\epsilon \\delta s}\\right\\rangle \\simeq \\left\\langle \\epsilon \\right\\rangle \\delta  s + O(\\delta s^2),\nor by integrating\n\\partial F/\\partial  s  =  -(kT/Z_{NVT}) \\partial Z_{NVT}/\\partial  s.\n\nThe TI formula has a precise interpretation as the integral of the\ngeneralized mean force; however, one needs to take care to ensure the\nquasi-adiabatic limit. In fact, the early single-configuration TI (SCTI)\nimplementation replaced the ensemble average by a single instantaneous\nvalue taken while changing s continuously but showed hysteresis \n\nMitchell & McCammon, 1991 at finite switching rates ds/dt, a clear\nsign of non-equilibrium. Multiconfiguration TI (MCTI) \n\nStraatsma & McCammon, 1991 addresses this issue by calculating proper ensemble\naverages at each value of s, yielding per-window statistical errors,\nand is also an embarrassingly parallel algorithm. Despite the\ndevelopment of more efficient methods, the flaw of implementing a\nnon-equilibrium sampling has sparked new interest in SCTI in the context\nof Jarzynski’s identity and steered molecular dynamics \n\nHummer, 2001\n\nPark & Schulten, 2004, see Section\n\n\nsec:single​-molecule​-spectroscopy on\nsingle molecule spectroscopy.\n\nWhen the perturbation affects an internal CV rather than an interaction\nparameter, the system has to be kept at or in the vicinity of the chosen\nvalue of \\xi=s. If the configuration change is driven by a\n\\xi-dependent perturbation in the form of a restraint V(q,\\xi),\ntypically a harmonic potential, the free energy of the unrestrained\nsystem is recovered by unbiasing the TI result with TP of the\nrestraint \n\nStraatsma & McCammon, 1991:\\Delta F(s)=\\int_{s_0}^{s}\\left\\langle \\frac{\\partial V}{\\partial \\xi}\\right\\rangle_{\\xi=s'}ds'\n+kT\\ln\\left\\langle e^{\\beta V(q,\\xi)}\\right\\rangle_{\\xi=s}\n\nAnother way of looking at the TI when the RC is a function of atomic\ncoordinates instead of a parameter of the Hamiltonian, for example\n\\xi(\\mathbf{r}) = \\left| \\mathbf{r}_2 - \\mathbf{r}_1 \\right|. In this\ncase the PMF along the RC in terms of the conditional probability p(s)\nis written asF(s) = -kT \\ln \\left\\langle \\delta\\left(\\xi(\\mathbf r) - s \\right)\\right\\rangle \\equiv - kT \\ln p(s),\n\nThe free energy difference takes the form\\Delta F = \\int_{s_1}^{s_2}  \\left\\langle \\frac{\\partial H}{\\partial \\xi}\\right\\rangle^\\mathrm{cond}_{\\xi=s} ds\n\nwhere the conditional average is defined by\\left\\langle  \\cdot \\right\\rangle^\\mathrm{cond}_{\\xi=s}=\\frac{\\left\\langle \\cdot \\delta\\left( \\xi -s \\right)\\right\\rangle}{\\left\\langle \\delta\\left( \\xi -s \\right)\\right\\rangle}.\n\nThis can easily be recovered as the stiff limit of a harmonic restraint.\nIn the case of holonomic constraints being imposed rather than\nrestraints, correct unbiasing requires more complex approaches described\nin Sec.\n\nsec:BM.\n\n\n\nFigure 1:Overcoming sampling limitations with biased approaches\nA, B. Unbiased simulations. When high free energy barriers hinder\ntransitions between metastable states, only local estimates of p(\\xi)\nare accurate.\nC, D. Umbrella Sampling. In US multiple restrained\nsimulations centered at reference positions \\xi_{ref} (red parabolas\nin C), enable a uniform coverage of the reaction coordinate, shown in D;\nthe sampled biased histograms are subsequently combined to recover the\nfull free energy profile (see Sec.\n\n\nsec:UmbrellaSampling).\nE, F. Metadynamics\nIn MetaD, the bias is history-dependent, progressively filling free\nenergy wells and promoting barrier crossing. The dynamics of \\xi(t),\nunder the effect of bias (panel F) shows how an ergodic exploration of\nconfiguration space is associated with bias construction (see Sec.\n\n\nsec:metadynamics).","type":"content","url":"/computing-biased-fep#thermodynamic-integration","position":9},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Umbrella Sampling"},"type":"lvl3","url":"/computing-biased-fep#umbrella-sampling","position":10},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Umbrella Sampling"},"content":"Umbrella sampling (US), introduced by Torrie and Valleau in the 1970s\n\n\nTorrie & Valleau, 1977\n\nKästner, 2011, is one of the earliest\nenhanced-sampling methods to compute an FES along a CV \\xi. US is a\nstatic bias method, i.e., the biasing potential does not change in\ntime. The central idea of US is to overcome the poor sampling of\nhigh-energy regions by introducing a ---usually harmonic--- restraint,\nwhich localizes the sampling around a reference value\n\\xi_i^{\\mathrm{ref}}:V_i(\\xi) = \\frac{1}{2} K (\\xi - \\xi_i^{\\mathrm{ref}})^2 .\n\n In the\ncontext of TP, V_i(\\xi) can be seen as the perturbation introduced\nin the physical Hamiltonian of the system in each of the i\nsimulations.\n\nIn US, a series of such biased simulations (“windows”, Fig.\n\n\nfig:biasedC) is performed to span the full range of \\xi\n(see Fig. \n\nfig:biasedD). Each window produces a biased\ndistribution p_i^b(\\xi). The unbiased distribution in window i is\nformally recovered asp_i(\\xi) = p_i^b(\\xi)  e^{\\beta V_i(\\xi)} \\langle e^{-\\beta V_i(\\xi)} \\rangle_0,\n\nfrom which the free energy (or potential of mean force, PMF) is obtained\nusing Eq. \n\neq:FES. Because each window only samples a narrow\nrange of \\xi, the problem of stitching the windows together arises:\nthe free-energy offsets between windows F_i are not known directly.\n\nSeveral post-processing strategies exist, all rooted in FEP and TI\nconcepts \n\nRoux, 1995. The Weighted Histogram Analysis Method (WHAM)\n\n\nKumar et al., 1992 is the de facto standard post-processing tool in\numbrella sampling and determines the free-energy offsets\nself-consistently by minimizing statistical error, merging all biased\nhistograms into a single distribution (see also \n\nSouaille & Roux, 2001\n\nHub et al., 2010. The Multistate Bennett\nAcceptance Ratio (MBAR) method provides an equivalent, statistically\noptimal alternative to WHAM \n\nShirts & Chodera, 2008\n\nTan et al., 2012.\n\nAn alternative that avoids explicit offset estimation is umbrella\nintegration (UI) \n\nKästner & Thiel, 2005, which reconstructs the mean\nforce directly from the biased distribution,\\frac{\\partial F}{\\partial \\xi} \\;=\\; -kT\\frac{\\partial \\ln p_i^b(\\xi)}{\\partial \\xi} - \\frac{dV_i}{d\\xi},\n\nyielding the PMF by integration over \\xi. This way, UI makes explicit\nthe proximity between US and thermodynamic integration methods,\neffectively implementing an FES estimator similar to Eq.\n\n\neq:TI_gen We note that UI inspired the development of Mean\nForce Integration, which can be applied to the post-processing of\nsampling gathered with both static and adaptive biasing methods\n\n\nMarinova & Salvalaglio, 2019\n\nBjola & Salvalaglio, 2024\n\nF. et al., 2024.\n\nOverall, umbrella sampling can be viewed as a restrained-sampling\nframework analyzed either by histogram reweighting (WHAM/MBAR) or by\nmean-force integration (UI); both approaches converge to the same PMF\ngiven sufficient sampling.\n\nOver the years US has inspired many extensions. Adaptive US iteratively\nrefines the bias toward uniform sampling \n\nMezei, 1987, while local elevation US introduces a history-dependent bias reminiscent of early metadynamics \n\nHuber et al., 1994\n\nHansen & Hünenberger, 2010\n\nLaio & Parrinello, 2002. Multidimensional\nformulations \n\nBartels & Karplus, 1997\n\nKästner, 2009 treat coupled coordinates, and hybrid schemes, such as self-learning adaptive\nvariants \n\nWojtas-Niziurski et al., 2013 or replica-exchange US \n\nJiang et al., 2012, enable efficient, massively parallel implementations. These developments confirm US remains a versatile and widely used framework for free-energy calculations.","type":"content","url":"/computing-biased-fep#umbrella-sampling","position":11},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Constrained reaction coordinates: the Blue Moon Ensemble"},"type":"lvl3","url":"/computing-biased-fep#constrained-reaction-coordinates-the-blue-moon-ensemble","position":12},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl3":"Constrained reaction coordinates: the Blue Moon Ensemble"},"content":"Often, holonomic constraints, where the constraining force acts along\n\\partial\\xi/\\partial \\mathbf{r} \n\nGoldstein et al., 2014\n\nVan Kampen & Lodder, 1984, are used as an alternative to harmonic\nrestraints to enforce a specific value of the CV \\xi(\\mathbf {r})=s\nusing algorithms like SHAKE \n\nRyckaert et al., 1977.\nIn this case, one needs to apply an unbiasing factor\nZ_{\\xi}= \\sum_i \\left|\\partial {\\xi}/\\partial \\mathbf r_i\\right|^2 / m_i\nto correct for momentum loss along the constraint \n\nCarter et al., 1989.\nCombining this factor with the contribution from the integration over\n(all, including the unbiased) kinetic degrees of freedom yields the Blue\nMoon ensemble configurational formula (so named because it helps sample\nevents that happen “once in a blue moon”) \\frac{dF}{ds}\n={\\displaystyle \\left\\langle Z^{-1/2}_\\xi\\right\\rangle^{-1}_{\\xi=s}}\n\\displaystyle \\left\\langle Z^{-1/2}_\\xi\\left[ \\frac{\\partial V}{\\partial \\xi}-\\frac{kT}{2}\\frac{\\partial \\ln|M(\\mathbf q)|}{\\partial \\xi}\\right]\\right\\rangle_{\\xi=s},\n\nwhere M is the mass-metric tensor appearing in Eq.\n\n\neq:metric​-factor​-origin. In this\nformulation, the RC must be one of the generalized coordinates used to\ndescribe the configurations. Sprick and Ciccotti derived an equivalent\nexpression that requires only the constraint force magnitude along with\na curvature correction \n\nSprik & Ciccotti, 1998. The case of\nmultidimensional reaction constants is discussed in Ref.\n\nCiccotti, 1991.\n\nUnbiasing constraints\n\nConstraints in the Blue Moon ensemble are\nartifacts and their bias clearly must be removed. Structural constraints\n(bond length, angles) can sometimes be corrected via the Fixman\npotential \n\nFixman, 1978. Yet, it is debatable whether this is\nappropriate, since both flexible and rigid bonds are approximations: the\nformer neglect high vibrational excitations, the latter ignore\nconformation-dependent zero-point effects \n\nVan Kampen & Lodder, 1984. In\naddition, as these bond length fluctuations are not independent modes,\nthey do not satisfy the general equipartition theorem\n\n\nTolman, 1918, which underpins the bias removal procedure.","type":"content","url":"/computing-biased-fep#constrained-reaction-coordinates-the-blue-moon-ensemble","position":13},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl2":"Adaptive Bias Methods: Metadynamics and derived methods"},"type":"lvl2","url":"/computing-biased-fep#sec-metadynamics","position":14},{"hierarchy":{"lvl1":"Computing Free Energy Surfaces","lvl2":"Adaptive Bias Methods: Metadynamics and derived methods"},"content":"A key ingredient of US is the use of a fixed set of windows chosen a\npriori, with convergence depending on their overlap. A complementary\nclass of methods instead builds the bias adaptively as the simulation\nprogresses. Metadynamics (MetaD), introduced by Laio and Parrinello \n\nLaio & Parrinello, 2002, is one of the most influential adaptive-bias\nalgorithms for reconstructing FESs \n\nBarducci et al., 2011\n\nLaio & Parrinello, 2002\n\nBussi & Laio, 2020\n\nValsson et al., 2016. Its central idea is to discourage a molecular\nsimulation from revisiting previously explored regions of the space of\nCVs \\xi; in doing so, metadynamics enhances the fluctuations along\n\\xi, thus speeding up the sampling \n\nValsson et al., 2016. A MetaD simulation evolves under a time-dependent bias potential\nV(\\xi,t) that is incrementally constructed as the trajectory\nprogresses. At regular time intervals, a small repulsive Gaussian hill\nof height w and width \\sigma is deposited at the instantaneous CV\nvalue \\xi(t): V(\\xi,t) = \\sum_{t'<t} w\ne^{-\\frac{\\left(\\xi-\\xi(t')\\right)^2}{2\\sigma^2 }}\n\n This “computational\nsand-filling” \n\nBarducci et al., 2011 progressively raises the free-energy of ensembles of configurations visited during sampling, allowing the system to escape local minima and visit new regions of phase space.\nIn the long-time limit, the accumulated bias offsets the underlying free-energy surface F(\\xi) up to an additive constant, so that V(\\xi,t \\to \\infty)\\approx -F(\\xi). Once this condition is reached, the biased dynamics samples a uniform probability distribution in CV space, effectively restoring ergodicity (see Sec. \n\nsec:Theory).\n\nMetaD is conceptually related to other history-dependent approaches,\nsuch as the local elevation method \n\nHuber et al., 1994. In particular, MetaD shares with local elevation the general principle of discouraging revisits to previously explored regions of collective variable space, while differing in its formulation and bias-update protocol. Moreover, compared with earlier mean-force-based schemes such as the Adaptive Biasing Force (ABF) method\n\n\nComer et al., 2015\n\nDarve & Pohorille, 2001\n\nDarve et al., 2002, metadynamics constructs the bias from local visitation history rather than explicit force estimates. Despite its simplicity and general applicability, MetaD suffers from a few drawbacks: the continual deposition of Gaussians can lead to systematic overshooting of the free-energy surface; convergence depends on the choice of Gaussian height and width; and the time dependence of V(\\xi,t) complicates rigorous reweighting.\nThese issues motivated the development of statistically controlled variants that address these shortcomings \n\nBarducci et al., 2008\n\nValsson et al., 2016\n\nBussi & Laio, 2020.\n\nWell-tempered metadynamics \n\nBarducci et al., 2008, is the\nmost popular variant of metadynamics that addresses several of these\nissues. For instance, WTMetaD introduces a smooth tempering of the bias\ndeposition rate to achieve self-limiting convergence. This is achieved\nby imposing a dynamic evolution of the Gaussian heights that decreases\nexponentially with the local value of the bias already accumulated:w(t) = w_0 e^\\frac{-\\beta{V(\\xi(t),t)}}{(\\gamma-1)}\n\n where\n\\gamma > 1 is the bias factor. At the beginning of a simulation, the\nGaussian bias height is w_0; as V(\\xi,t) grows, the added bias\ndiminishes, so that V asymptotically approaches a smoothed version of\nthe underlying FES as V(\\xi,t\\to\\infty) = -({1-1/\\gamma}) F(\\xi). The\nbias factor \\gamma tunes the trade-off between exploration (large\n\\gamma) and accuracy (small \\gamma). The stationary distribution\nsampled by WTMetaD in the long-time limit is no longer flat but\nwell-tempered, i.e. p(\\xi)\\propto {e^{-\\beta F(\\xi)/\\gamma}}. The\nsampling along \\xi therefore occurs at an effectively elevated\ntemperature T_\\text{eff}=\\gamma T, which thus enhances barrier\ncrossing while maintaining a known, analytically recoverable bias.\nWTMetaD has, de facto, become the standard formulation implemented in\nmodern software (e.g., PLUMED \n\nTribello et al., 2014, GROMACS \n\nLindahl et al., 2022, LAMMPS \n\nThompson et al., 2022) because it provides controlled\nconvergence, improved statistical efficiency, and the possibility to\nmonitor the flattening of the free-energy landscape on-the-fly.\n\nOne can combine WTMetaD with static biases (e.g., harmonic restraints,\nwalls, or custom static biases) in a straightforward, additive manner\n\n\n@Awasthi2016, Limongelli et al., 2013, @MFI_Bjola. If the WTMetaD bias is deposited sufficiently\nslowly - so that transition states remain effectively bias-free - the\ninfrequent metadynamics framework allows barrier-crossing times to be\nrescaled to recover physical rate constants \n\nTiwary & Parrinello, 2013\n\nSalvalaglio et al., 2014\n\nPalacio-Rodriguez et al., 2022. The bias potential accumulated during a MetaD or WTMetaD\nsimulation modifies the underlying probability distribution, so that\ndirect estimates of the free energy F(\\xi) from the bias V(\\xi,t)\nare inherently time-dependent. Several formulations have been proposed\nto obtain time-independent free-energy estimators, which recover\nF(\\xi) or the distribution of other observables from a trajectory\nsampled under the effect of a time-dependent bias \n\nTiwary & Parrinello, 2015\n\nBonomi et al., 2009\n\nMarinova & Salvalaglio, 2019\n\nGiberti et al., 2019\n\nOno & Nakai, 2020.\n\nRecent developments have further generalized ideas originating from\nmetadynamics by recasting bias construction in probabilistic or\nvariational terms, giving rise to methods such as Variationally Enhanced\nSampling (VES) \n\nValsson & Parrinello, 2014, On-the-fly Probabilty\nEnhanced Sampling (OPES) \n\nInvernizzi et al., 2020, and GAMBES \n\nDebnath & Parrinello, 2020 which differ in how the target distribution is enforced. VES does so by minimizing a KL-based variational functional. In contrast, OPES\nachieves it via an explicit on-the-fly estimate of the marginal biased\nprobability density and a reweighting-based update of the bias; GAMBES,\ninstead, reconstructs the bias from a Gaussian Mixture approximation of\nthe sampled probability distribution. We refer the reader interested in\nadditional details to the comprehensive review of Henin et al. \n\nHénin et al., 2022.","type":"content","url":"/computing-biased-fep#sec-metadynamics","position":15},{"hierarchy":{"lvl1":"Reaction Paths on the Free Energy Landscapes: Transition Path Sampling"},"type":"lvl1","url":"/computing-pathbased","position":0},{"hierarchy":{"lvl1":"Reaction Paths on the Free Energy Landscapes: Transition Path Sampling"},"content":"As mentioned in Sec.\n\nsec:kinetics, FEPs can be used to\nextract useful information about reaction kinetics, but care must be\ntaken in presence of complex landscapes. A complementary class of\napproaches focuses on determining representative transition pathways by\noptimizing geometric or variational principles. Across these families of\nmethods, algorithms inspired by minimum‐action geometry - such as Nudged Elastic Band and its extensions [\n\nJónsson et al. (1998); \n\nHenkelman & Jónsson (2000);wang2016automated; \n\nÁsgeirsson et al. (2021)] as well as\nfinite-temperature string approaches [weinan2002string; weinan2005finite; \n\nE et al. (2007)] and Onsager-Machlup formulations \n\nOnsager & Machlup, 1953\n\nAdib, 2008\n\nFaccioli et al., 2006 - contrast with flux-based kinetic formalisms such as the Bennett-Chandler theory [\n\nRuiz-Montero et al. (1997); \n\nMaragliano et al. (2006); vanden2010transition] in that the former determine representative transition pathways by extremizing geometric or variational principles, whereas the latter quantify\nreaction rates through time-correlation functions and dynamical recrossing statistics.\n\nThese geometric and variational approaches\nprovide valuable insight into likely pathways, but a complete statistical description of rare events requires sampling full reactive trajectories, as achieved by Transition Path Sampling (TPS). For a\nMarkovian dynamics, the probability density of a trajectory\n\\{\\mathbf{r}\\}=(\\mathbf{r}_0,\\mathbf{r}_1,\\dots,\\mathbf{r}_t) isp[\\{\\mathbf{r}\\}] = \\rho(\\mathbf{r}_0)\\prod_{i=0}^{t-1}p(\\mathbf{r}_{i+1}|\\mathbf{r}_i),\n\nwhere \\rho(\\mathbf{r}_0) is the equilibrium distribution and\np(\\mathbf{r}_{i+1}|\\mathbf{r}_i) the conditional propagator \n\nBolhuis et al., 2002. The probability that a trajectory initiated in A reaches\nB at time t isC(t) = \\frac{\\langle h_A(\\mathbf{r}_0),h_B(\\mathbf{r}_t)\\rangle}{\\langle h_A(\\mathbf{r}_0)\\rangle}\n= \\frac{\\mathcal{Z}_{AB}(t)}{\\mathcal{Z}_A},\n\n with\\mathcal{Z}_A = \\int\\mathcal{D}\\{\\mathbf{x}\\}h_A(\\mathbf{r}_0)p[\\{\\mathbf{r}\\}]\\qquad\n\\mathcal{Z}_{AB} = \\int \\mathcal{D}\\{\\mathbf{r}\\} h_A(\\mathbf{r}_0)p[\\{\\mathbf{r}\\}]h_B(\\mathbf{r}_t),\n\nwhere \\mathcal{Z}_A is the probability of a trajectory to start from\n\\mathbf{r}_0, regardless of where it ends at time t, and\n\\mathcal{Z}_{AB} that to start in r_0 and end in r_t. These\nprobabilities are expressed in terms of path integrals, effectively\nfunctional integrals over the set of all possible paths\n\n\nFeynman, 1948\n\nKleinert, 2009\n\nSeifert, 2012. The rate constant, for example, follows from the long-time derivative \n\nBolhuis et al., 2002, k_{AB} = \\lim_{t\\to\\infty}\\frac{d}{dt}\\left(\\frac{Z_{AB}(t)}{Z_A}\\right),\n\n and the problem of evaluating the rate k_{AB} reduces to sampling the path ensemble in the same conceptual way as equilibrium properties are obtained from the Boltzmann distribution.\n\nOne can generate trajectories that sample the biased probability of starting in region A and ending\nin region B without recrossing in a straightforward manner. Starting\nfrom one reactive path, new trial trajectories \\{\\mathbf{r}'\\} are\ngenerated by small stochastic modifications and integrated forward and\nbackward in time, accepting or rejecting the new path \\{\\mathbf{r}'\\}\nusing a Metropolis scheme \n\nMetropolis & Ulam, 1949\n\nKalos & Whitlock, 2008\n\nFrenkel & Smit, 2023.\nIn the microcanonical ensemble or other generalized ensembles with extended Hamiltonians, the acceptance\nprobability becomes remarkably simple \n\nDellago et al., 2002 as all reactive proposals are accepted.\nQuantities like the rate\nk_{AB} are not directly accessible as they are computed with a normalization factor over the path starting in A and ending at any\npoint. However, TPS provides direct access to the transmission function\n\\kappa(t) = k(t)/k_{tst} via\n\\kappa(t) = \\langle \\dot h_B(t) \\rangle_{AB} / \\langle \\dot h_B(0) \\rangle_{AB}.\nThe rate constant can be accessed by supplementing TPS with a US in the\nTPE \n\nDellago et al., 1999. Sampling the TPE enables a\nstraightforward computation of the committor function p_B(\\mathbf r)\n(see Sec.\n\nsec:committor). With TPS one can easily\nidentify transition-state configurations by launching short trajectories\nfrom frames along the generated paths and selecting those for which\np_B\\simeq{0.5}.\n\nRelated path-sampling approaches include Transition Interface Sampling (TIS) \n\nVan Erp et al., 2003\n\nVan Erp & Bolhuis, 2005 Forward Flux Sampling (FFS) \n\nAllen et al., 2009, and Markov State Models (MSMs) \n\nPrinz et al., 2011\n\nChodera & Noé, 2014.\n\nTIS introduces a hierarchy of interfaces between A and B, estimating the rate as the product of conditional crossing probabilities and the initial flux.\nFFS instead propagates trajectories forward across\ninterfaces, making it suitable for non-equilibrium or irreversible systems.\nMSMs reconstruct long-time kinetics from short unbiased\ntrajectories by estimating transition probabilities between metastable states, with rates and committors derived from the transition matrix.\n\nFurther refinements include Precision, S-, and Aimless Shooting \n\nGrünwald et al., 2008\n\nMenzl et al., 2016\n\nPeters & Trout, 2006, and hybrid schemes combining TPS with metadynamics or replica exchange \n\nBorrero & Dellago, 2016\n\nVan Erp, 2007. A\ndetailed overview is given in \n\nBolhuis & Swenson, 2021.","type":"content","url":"/computing-pathbased","position":1},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Free Energy Surfaces (FESs) are low-dimensional representations of the\nthermodynamic stability of ensembles of atomic configurations. As\nsuch, FESs are valuable tools for mapping, rationalising, and navigating\nthe inherent complexity of high-dimensional datasets generated by\nsampling atomistic models through simulations that build on Molecular\nDynamics and Monte Carlo techniques \n\nFrenkel & Smit, 2023\n\nTuckerman, 2023\n\nChandler, 1987.\nEnabling a direct connection between the\nmicroscopic world of atomic configurations and interpretable,\nmacroscopic observables, the ability to compute, read, and interpret\nFESs is crucial for the quantitative interpretation of atomistic\nsimulation results, and thus also for the adoption of molecular\nsimulation techniques in an engineering context. Our goal in this review\nis to provide a route to computing, reading, and interpreting FESs,\nfirst by establishing the theoretical foundations, then by surveying\npractical methods for their calculation, and finally by demonstrating\nhow modern machine learning enhances both the representation and\nsampling of FESs. Section \n\nsec:Theory introduces the\nstatistical--mechanical basis of FESs, linking partition functions and\nthermodynamic potentials to marginal probabilities, and showing how\nprojections onto collective variables (CVs) yield interpretable\nlandscapes. Key aspects include ergodicity, CV selection, and how FESs\nprovide barrier heights and equilibrium constants. Section\n\n\nsec:Computing outlines practical computational\ntechniques aimed at evaluating FESs from molecular simulations. Section\n\n\nsec:MLCVs explores machine learning in the analysis and\ncalculation of FESs, providing an overview of recent developments, from\nmachine-learned CVs and variational committor models to ML-enhanced\ntransition-path sampling, and highlights how learning and sampling can\nbe integrated to improve the representation and exploration of molecular\nprocesses.","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"Molecular Understanding of Free Energy Landscapes"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Molecular Understanding of Free Energy Landscapes"},"content":"Free energy surfaces (FESs) offer a unifying framework for understanding\nmolecular-level structures, transformations and thermodynamic stability.\nThey distill the complexity of atomistic simulations into interpretable\nlandscapes of metastable states, bridging molecular-level detail with\nmacroscopic observables. This review provides researchers in molecular\nsimulations, computational physical chemistry, and chemical engineering\nwith a conceptual and practical guide to computing and interpreting\nFESs, from their statistical-mechanical foundations to modern machine\nlearning approaches that are transforming the sampling, representation\nand analysis of molecular systems.\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Molecular Understanding of Free Energy Landscapes","lvl2":"Table of Contents"},"type":"lvl2","url":"/#table-of-contents","position":2},{"hierarchy":{"lvl1":"Molecular Understanding of Free Energy Landscapes","lvl2":"Table of Contents"},"content":"Molecular Understanding of Free Energy Landscapes\n\nIntroduction\n\nTheory: Defining a Free Energy Surface\n\nComputing Free Energy Surfaces\n\nFree Energy Profiles and Equilibrium Constants\n\nFESs from experiments: single molecule force spectroscopy\n\nReaction Paths on the Free Energy Landscapes: Transition Path Sampling\n\nMachine-Learned Collective Variables and Their Associated Free Energy Surfaces\n\nSummary and Conclusions\n\nMSa gratefully acknowledges the HT-MATTER UKRI Frontier Research Guarantee Grant (EP/X033139/1) for funding, and all the members of the Molecular Modelling and Engineering group who offered feedback on earlier versions of this manuscript, in particular Aaron Finney, Matteo Paloni, and Florian M. Dietrich.\n\n","type":"content","url":"/#table-of-contents","position":3},{"hierarchy":{"lvl1":"Summary and Conclusions"},"type":"lvl1","url":"/summary-and-conclusions","position":0},{"hierarchy":{"lvl1":"Summary and Conclusions"},"content":"Over the past few decades, the calculation of free-energy landscapes has\nevolved from a niche activity within molecular thermodynamics to a\nstandard component of molecular simulation workflows, driven by\nincreasing computing power and a proliferation of established\nalgorithms. At the same time, each method has its own assumptions and\nsubtleties, and no single approach is universally applicable, so it can\nbe difficult for newcomers to navigate this landscape. This review aims\nto provide a structured overview of the main families of methods and key\nreferences, while also highlighting how emerging machine-learning\napproaches are extending the scope of free-energy calculations.\n\nBy linking statistical mechanics with molecular simulations, we connect\nmicroscopic configurations to macroscopic thermodynamic observables,\nsupporting the growing use of molecular modelling in chemical and\nbiochemical engineering \n\nBurcham et al., 2024. We show how FESs can be\nutilized to map equilibrium probabilities into interpretable\nrepresentations of stability, and how their meaning depends on the\nchoice of CVs that capture the essential transformation coordinates.\nMachine learning now enhances FES calculations by discovering CVs,\noptimizing biases, and learning mean forces while preserving\nthermodynamic consistency, providing exciting new research avenues and\nmethodological improvements. The principles summarized here aim to equip\nreaders with a transferable foundation for applying and advancing\nmolecular simulation as a quantitative design tool in molecular and\nprocess engineering.","type":"content","url":"/summary-and-conclusions","position":1},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface"},"type":"lvl1","url":"/theory","position":0},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface"},"content":"","type":"content","url":"/theory","position":1},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Thermodynamic Potentials and Partition Functions"},"type":"lvl2","url":"/theory#thermodynamic-potentials-and-partition-functions","position":2},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Thermodynamic Potentials and Partition Functions"},"content":"In the canonical ensemble, which describes a system at constant number\nof particles (N), volume (V), and temperature (T), the probability\ndensity f(\\mathbf{r}, \\mathbf{p}) of finding the system in a\nparticular microstate characterized by the 6N-dimensional phase space\nvector of atomic (Cartesian, for simplicity) coordinates and momenta\n\\Gamma = (\\mathbf{r},\\mathbf{p}) is proportional to the Boltzmann\nfactor e^{-\\beta H(\\mathbf{r},\\mathbf{p})}, which measures the total\nenergy of the microstate, expressed by the Hamiltonian H, in units of\nthe thermal energy kT = 1/\\beta, k_\\mathrm{B} being the Boltzmann\nconstant and T the absolute temperature \n\nChandler, 1978\n\nFrenkel & Smit, 2023\n\nTuckerman, 2023. The normalization constant of the\nphase-space density f(\\mathbf{r}, \\mathbf{p}) is the partition\nfunction Z_{NVT}, which is, in essence, a Boltzmann-weighted count of\nthe number of accessible microstates and enables a direct connection\nbetween molecular configurations (i.e., realizations of\n\\mathbf{r}, \\mathbf{p}), and thermodynamic concepts familiar to an\nengineering audience.\n\nThe partition function in the canonical ensemble for a system of N\nidentical particles is written as:Z_{NVT} = \\frac{1}{N! h^{3N}}\\int e^{-\\beta H(\\Gamma)}d\\Gamma .\n\n Two inherently quantum-mechanical terms appear\nin the normalization factor: the count of permutations of identical\nparticles N! and the phase space volume unit, h^{3N}, set by the\nuncertainty principle, which are necessary to obtain an extensive\nentropy, avoid mixing entropy (Gibbs) paradoxes [\n\nNoyes (1961); (Noyes 1961; \n\nFrenkel & Smit (2023); \n\nTuckerman (2023)] and recover the Sackur-Tetrode entropy of the\nideal gas \n\nHuang, 1987. Following convention, we will use angular\nbrackets to denote the canonical ensemble average of an observable\nO(\\Gamma), \\langle O \\rangle\n= \\int O(\\Gamma)  f(\\Gamma)d\\Gamma\n= \\frac{1}{Z_{NVT}} \\int  O(\\Gamma)  e^{-\\beta H(\\Gamma)}d\\Gamma .\n\nUseful definitions\n\nPhase Space VectorIndicated with \\Gamma, it is, for a system\nwithout constraints, a 6N-tuple that encompasses the positions\n\\mathbf{r}=(\\mathbf{r}_1,\\ldots\\mathbf{r}_N) and momenta\n\\mathbf{p}=(\\mathbf{p}_1,\\ldots\\mathbf{p}_N) of all N particles in a\nsystem. Here, we represent this as: \\Gamma = (\\mathbf{r},\\mathbf{p})\n.\n\nTuckerman, 2023\n\nMicrostate A specific realization of the\nphase space vector \\Gamma represents a single point in the\n6N-dimensional phase space, defining the complete microscopic state of a\nclassical system at a given instant in time. \n\nTuckerman, 2023\n\nHamiltonianIndicated with H(\\Gamma), it is a fundamental function\nin classical mechanics that describes the total energy of a system as a\nfunction of its generalized coordinates and conjugate momenta.\n\n\nTuckerman, 2023\n\nIf the Hamiltonian can be written as the sum of independent kinetic and\npotential energy terms, H(\\Gamma) = K(\\mathbf{p}) + U(\\mathbf{r}),\nmomenta can be integrated out and the partition function can be\nexpressed in terms of the configurational integralQ_{NVT} = \\int  e^{-\\beta U(\\mathbf{r})}d\\mathbf{r},\n\n as Z_{NVT}\n=\\frac{1}{N! \\Lambda^{3N}} Q_{NVT},\n\n where\n\\Lambda = h/\\sqrt{2\\pi m kT} is the thermal wavelength for particles\nof mass m. The configuration (marginal) probability density function\nis in this case \n\nTuckerman, 2023:f(\\mathbf{r}) = \\frac{e^{-\\beta U(\\mathbf{r})}}{Q_{NVT}} = \\frac{e^{-\\beta U(\\mathbf{r})}}{\\int  e^{-\\beta U(\\mathbf{r})}d\\mathbf{r}}\n\nUseful Definitions\n\nZ or Q?\nThere is often confusion about which symbol refers to the\nfull partition function, and which to the configurational one, and for\ngood reasons! While IUPAC \n\nBrett et al., 2023lists both Z and Q\nas possible symbols for the partition function, without specifying\nwhether it is the full or the configurational one, classic textbooks\nincluding Huang \n\nHuang, 1987, Frenkel and Smit\n\n\nFrenkel & Smit, 2023, and Allen and Tildesley\n\n\nAllen & Tildesley, 2017, use Q for the full partition function, while\nRowlinson and Widom \n\nRowlinson & Widom, 2002, Kittel\n\n\nKittel, 2004 and Landau and Lifshitz \n\nLandau & Lifshitz, 1980 use Z.\nHere, we follow the latter convention.\n\nIsothermal-Isobaric Ensemble\nThe isothermal--isobaric partition function is related to the canonical\npartition function via \n\nTuckerman, 2023:\n\\Delta_{NPT}=\\int_0^\\infty  e^{-\\beta PV}Z_{NVT}dV","type":"content","url":"/theory#thermodynamic-potentials-and-partition-functions","position":3},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"From Partition Functions to Thermodynamic Potentials","lvl2":"Thermodynamic Potentials and Partition Functions"},"type":"lvl3","url":"/theory#from-partition-functions-to-thermodynamic-potentials","position":4},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"From Partition Functions to Thermodynamic Potentials","lvl2":"Thermodynamic Potentials and Partition Functions"},"content":"The partition function Z is a central quantity because it encodes all\nthe thermodynamic information of the system in this ensemble. In the\ncanonical ensemble, the thermodynamic potential, known as Helmholtz free\nenergy, A, can be written as: A = -kT \\ln Z_{NVT},\n\n or, in terms\nof the configuration integral,A = -kT \\ln Q_{NVT}  + kT\\ln\\left( N!\\Lambda^{3N}\\right),\n\nwhere the first term on the right-hand side is the\nconfigurational free energy and the second one the translational free\nenergy. Other thermodynamic quantities (i.e., those that depend only on\nthe macroscopic control parameters N,V,T) can then be computed as\nderivatives of A. Other statistical ensembles can be derived, for\ninstance, from the canonical ensemble, through a Legendre transformation\nwith respect to one control variable. This operation yields new\ndistribution functions, corresponding to the Laplace transform of the\noriginal one. In what follows, we use the symbol F to refer to the\nfree energy or thermodynamic potential without connection to an ensemble\nin particular. The specific meaning of F - whether Helmholtz or Gibbs\nfree energy - depends on the statistical ensemble used to sample\nmolecular configurations and compute free energy surfaces, as discussed\nin Section \n\nsec:Computing.","type":"content","url":"/theory#from-partition-functions-to-thermodynamic-potentials","position":5},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"From Thermodynamic Potential to Free Energy Differences","lvl2":"Thermodynamic Potentials and Partition Functions"},"type":"lvl3","url":"/theory#from-thermodynamic-potential-to-free-energy-differences","position":6},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"From Thermodynamic Potential to Free Energy Differences","lvl2":"Thermodynamic Potentials and Partition Functions"},"content":"Now, let us consider two disconnected regions of the phase space,\n\\Omega_A and \\Omega_B, representing two sets of microstates\nassociated with two distinct states of the system, A and B. Such\nensembles of configurations could correspond to the reactants and\nproducts of a chemical reaction, two different phases of the same\nsubstance, the unfolded and folded configurations of a biopolymer, etc.\nBy integrating the normalized canonical phase space distribution within\n\\Omega_A (or \\Omega_B), one can obtain the equilibrium probability\nto observe the system in state A (or B), and thus the free energy\ndifference between the two states:\\Delta{F}_{A\\rightarrow B}=-\nkT\\ln\\left[{\\frac{\\int\n{\\mathbf{1}_{\\mathbf{r}\\in{\\Omega_B}}f(\\mathbf{r})}d\\mathbf{r}}{\\int\n{\\mathbf{1}_{\\mathbf{r}\\in{\\Omega_A}}f(\\mathbf{r})}d\\mathbf{r}}}\\right]\n\n where \\mathbf{1}_{\\mathbf{r}\\in{A}},\n\\mathbf{1}_{\\mathbf{r}\\in{B}} are indicator functions selecting only\nmicrostates belonging to state A, and B, respectively. Integrating\nthe indicator functions \\int\n{\\mathbf{1}_{\\mathbf{r}\\in{\\Omega_A}}f(\\mathbf{r})}d\\mathbf{r}, \\int\n{\\mathbf{1}_{\\mathbf{r}\\in{\\Omega_B}}f(\\mathbf{r})}d\\mathbf{r}) gives\naccess to the equilibrium probability of states A and B.\n\n\n\nFigure 1:From microscopic configurations to the free energy surface. Each\nmolecular configuration (\\mathbf{r}_i \\in \\mathbf{R}^{3N}, where N\nis the number of atoms) is mapped to a point in a reduced space of CVs\n(\\xi(\\mathbf{r}) \\in \\mathbf{R}^m), with m \\ll 3N. The marginal\nequilibrium probability p(\\xi), which quantifies the relative\nlikelihood of observing configurations consistent with a given value of\n\\xi=[\\xi_1,\\xi_2]. The corresponding free energy surface\nF(\\xi) = -kT \\ln p(\\xi) + C provides a readable map of thermodynamic\nstability and metastability in configuration space. Basins A and B\ncorrespond to metastable states separated by a free energy\nbarrier.","type":"content","url":"/theory#from-thermodynamic-potential-to-free-energy-differences","position":7},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Free Energy Surfaces: readable maps of the thermodynamic potential"},"type":"lvl2","url":"/theory#free-energy-surfaces-readable-maps-of-the-thermodynamic-potential","position":8},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Free Energy Surfaces: readable maps of the thermodynamic potential"},"content":"Evaluating the equilibrium probability of state i, p_{i}, involves\ndefining the indicator function \\mathbf{1}_{\\mathbf{r}\\in i}, which\nidentifies configurations in \\Omega_{i}. Due to the high\ndimensionality of \\mathbf{r}, this is challenging and typically\naddressed by introducing a low-dimensional mapping \\xi(\\mathbf{r})\nthat groups similar microstates and allows defining the marginal\nequilibrium probability p(\\xi) as \n\nKirkwood, 1935\n\nFrenkel & Smit, 2023\n\nTuckerman, 2023:p(\\xi)=\\int{f(\\mathbf{r})\\delta({\\xi({\\mathbf{r}})-\\xi})d\\mathbf{r}}\n\n where p(\\xi) is the equilibrium probability\ndensity of the ensemble of microstates mapping to the same value of\n\\xi(\\mathbf{r}). In this context, p(\\xi) can be interpreted as the\n(normalized) partition function associated with all the microstates\nmapped in \\xi. In the following, to favor readability, we indicate\n\\xi as a scalar; however, its dimensionality can be higher than one,\nand extensions to higher dimensionality are straightforward(Laio and\nParrinello 2002; Kästner 2011).\n\nIn analogy with Eq.\n\neq:FE we can define a free energy for every\n\\xi as: F(\\xi)=-kT\\ln{p(\\xi)}+C\n\n where F(\\xi) is the FES, and C is an arbitrary\nconstant, indicating that F(\\xi) is a measure of relative\nthermodynamic stability between ensembles of states that map to\ndifferent values of \\xi.\n\nMapping configurations onto a physically meaningful \\xi such that,\ni.e., it captures slow transitions in the configurational ensemble (see\nSection \n\nsec:CVs), renders the features of F(\\xi)\ninformative. For instance, for a good choice of \\xi, metastable states\ncorrespond to local minima in F(\\xi). As a consequence, free energy\ndifferences between metastable states become tractable as the domain of\nintegration (\\Omega_i in Eq. \n\neq:DF) can be identified in\nreduced-dimensionality \\xi (see Fig. \n\nfig:FES_idea).\n\nA subtle but important point is that FESs are low-dimensional\nprojections of configurational space, where each value of \\xi\nrepresents an ensemble of possibly distinct microstates whose degeneracy\ncontributes to the configurational entropy S(\\xi) \n\nGimondi et al., 2018\n\nDietschreit et al., 2023.\nBroadly degenerate regions have higher entropy, while narrowly defined\nones follow the potential energy U(\\xi) more closely. The internal\nenergy contribution to an FES is obtained as a conditional ensemble\naverage over all microstates compatible with the value of \\xi:U(\\xi) = \\langle U(\\mathbf{r}) \\rangle_{\\xi} =\n \\frac{\\int  U(\\mathbf{r}) e^{-\\beta U(\\mathbf{r})} \\delta(\\xi(\\mathbf{r})-\\xi)d\\mathbf{r}}{\\int  e^{-\\beta U(\\mathbf{r})} \\delta(\\xi(\\mathbf{r})-\\xi)d\\mathbf{r}}\n\nThe entropy surface, then follows from:\nS(\\xi) = T^{-1}(U(\\xi) - F(\\xi)).\n\nAs illustrated in Fig. \n\nfig:entropy, basins with\nidentical potential energies can differ in free energy purely due to\nentropy, an effect crucial in biomolecular and soft-matter systems where\nconformational transitions and barriers often reflect entropic, rather\nthan energetic, contributions \n\n@gimondi2018building, polino2020collective, Kollias et al., 2020, @leanza2023into, @Serse2024.\n\n\n\nFigure 2:Entropy and Energy surfaces in \\xi\nA. Two-dimensional model potential energy surface, U(x,y), and corresponding projection on the map variable (x) used to illustrate the decomposition of the free energy surface into energetic and entropic contributions, following\nRef. (Gimondi, Tribello, and Salvalaglio 2018). The potential energy\nlandscape features two basins of comparable depth (A and B) but markedly\ndifferent widths along the coordinate y. B. One-dimensional\nprofiles of the free energy \\Delta F(x) (blue), average potential\nenergy \\Delta U(x) (red), and entropic term -T \\Delta S(x) (green).\nAlthough the two minima have identical \\Delta U(x), basin B exhibits a\nlower free energy because of its higher degree of configurational\ndegeneracy in y, associated with higher entropy. Reproduced with\npermission from Ref. \n\nGimondi et al., 2018.","type":"content","url":"/theory#free-energy-surfaces-readable-maps-of-the-thermodynamic-potential","position":9},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"type":"lvl2","url":"/theory#collective-variables-order-parameters-and-reaction-coordinates","position":10},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"content":"The central role of \\xi is to provide a reduced representation of the\nhigh-dimensional configuration space that retains the essential ability\nto distinguish relevant metastable states, and slow transition modes\nbetween them, for the molecular process of interest \n\nKirkwood, 1935\n\nFrenkel & Smit, 2023, @tuckerman2023statistical. Depending on the field of application, the characteristics of the studied process, and its own properties, the low-dimensional mapping \\xi can be referred to by different names. The most common are collective variables (CVs), order parameters (OPs), and reaction coordinates (RCs) \n\nHénin et al., 2022.\nAlthough these terms overlap partially and are sometimes used\ninterchangeably by practitioners, they imply some fundamental\ndistinctions. Clarifying and understanding their differences is thus\ncrucial for a consistent interpretation of FESs associated with\nmolecular transformations.\n\n\n\nFigure 3:A. Reaction Coordinates and Collective Variables: Lessons from\nIon-Pair Dissociation in Water.\nThe distinction between CVs and RCs is\nboth conceptual and practical. An early and historically influential\nexample illustrating this difference is the dissociation of a\nNa^+Cl^- ion pair in water, investigated by Geissler et al. \n\nGeissler et al., 1999. As illustrated by Geissler,\ntwo landscapes F(r_{\\text{ion}}, q_S) with identical projections\nF(r_{\\text{ion}}) can imply distinct mechanisms. In the simplest\ncase,(a), the barrier in F(r_{\\text{ion}}) defines the transition\nstate, making r_{\\text{ion}} not only a good CV but also a RC.\nHowever, when solvent reorganization adds an orthogonal coordinate\nq_S, b, configurations at r_{\\text{ion}} = r^* belong to stable\nbasins rather than the transition region, so r_{\\text{ion}} remains a\ngood CV (able to distinguish dissociated from undissociated) but it is\nnot a good RC.\n\nB. Choosing the Right Coordinate: The Ring Puckering Example.\nThe choice of CVs is critical in free-energy calculations, but not always\nobvious. Puckered ring conformers can be described by the Cremer--Pople\nCartesian coordinates, obtained from the out-of-plane displacements\nz_j of the 6-membered ring atoms as\nq_x =  \\sum_{0}^5 z_j \\cos\\left[\\frac{2\\pi}{3}j\\right]/\\sqrt{3},\nq_y = -\\sum_{0}^5 z_j \\sin\\left[\\frac{2\\pi}{3}j\\right]/\\sqrt{3},\n\\quad q_z = \\sum_{0}^5 (-1)^j z_j/ \\sqrt{6}, or by their polar\nrepresentation\n\\left(Q \\sin \\theta \\cos \\phi, Q \\sin \\theta \\sin \\phi, Q\\cos \\theta\\right)\n(left panel). Only (\\theta,\\phi) are effective variables for biasing,\nas they capture conformer connectivity. Using two polar coordinates,\nmetadynamics spans the entire puckering free-energy landscape for\nglucuronic acid (middle panel, 5 kJ/mol isolines), whereas biasing along\nCartesian projections\n(q_x,q_y)=(Q\\sin\\theta\\cos\\phi,Q\\sin\\theta\\sin\\phi) fails near the\nequatorial line. There, ergodicity is broken as the bias acts only\nperpendicularly to the puckering sphere (right panel). \n\nSega et al., 2009.","type":"content","url":"/theory#collective-variables-order-parameters-and-reaction-coordinates","position":11},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"Collective Variables and Order Parameters","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"type":"lvl3","url":"/theory#collective-variables-and-order-parameters","position":12},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"Collective Variables and Order Parameters","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"content":"A CV is the most general of the three denominations: it is any function\nof the atomic coordinates designed to reduce the enormous dimensionality\nof a molecular system into a smaller, more interpretable set of\ndescriptors. To be useful, CVs must distinguish all the relevant\nlong-lived metastable states involved in a transformation, i.e., the\nreactants and the products. In this case, the metastable states of\ninterest will appear as local maxima in p(\\xi), and local minima in\nthe FES, F(\\xi). Typical CVs include simple geometrical descriptors,\nsuch as distances and angles \n\nHénin et al., 2022\n\nFiorin et al., 2013\n\nBonomi et al., 2019\n\nTribello et al., 2025, as well as more\ncomplex functions, including measures of structural similarity\n\n\nPietrucci & Laio, 2009 or progress along a path defined by a set of\nreference structures \n\nBranduardi et al., 2007. It\nshould be noted that CVs do not necessarily require a direct physical\ninterpretation, and they can be abstract or highly engineered \n\nPietrucci & Andreoni, 2011.\nFor instance, combinations of distances, angles, or\nlatent variables from dimensionality-reduction algorithms can be\neffective CVs by allowing for a clear distinction between metastable\nstates \n\nTribello et al., 2012, while losing a direct physical\ninterpretability (see an extended discussion in section\n\n\nsec:MLCVs).\n\nOPs are a specific type of CV introduced in\nstatistical mechanics to distinguish between different thermodynamic\nphases or states of matter \n\nNeha et al., 2022\n\nDesgranges & Delhommelle, 2025\n\nGiberti et al., 2015. OPs typically reflect\na symmetry-breaking or structural feature that changes qualitatively at\na phase transition---for example, density in liquid--gas coexistence,\norientational alignment in liquid crystals, and roto-translational\ninvariance in crystalline systems \n\nSteinhardt et al., 1983\n\nTribello et al., 2017\n\nGimondi & Salvalaglio, 2017\n\nPiaggi & Parrinello, 2017. Although OPs are often used to obtain a global\ndescription of an atomistic system, they are typically constructed from\nlocal contributions within well-defined atomic environments (Lechner and\nDellago 2008; Bartók, Kondor, and Csányi 2013; Piaggi and Parrinello\n2017; Giberti, Salvalaglio, and Parrinello 2015; Caruso et al. 2025).\nWhen dealing with characterising the state of molecular solids, OPs\nbased on measures of similarity between distributions capturing the\ntranslational, orientational, and conformational order are particularly\neffective \n\nGobbo et al., 2018\n\nGimondi & Salvalaglio, 2017\n\nFrancia et al., 2020.","type":"content","url":"/theory#collective-variables-and-order-parameters","position":13},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"Reaction Coordinates","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"type":"lvl3","url":"/theory#reaction-coordinates","position":14},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"Reaction Coordinates","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"content":"An RC implies a further specialization: it is a low-dimensional\ndescriptor intended to capture the progress of the most probable\ntransition pathway between reactants and products. An ideal RC is not\nonly correlated with the transition but also uniquely parameterizes the\nprogress of the reaction and identifies the transition state\nensemble(Vanden-Eijnden 2006; Peters and Trout 2006; Peters 2017). An\nimportant point to note is that, when (a combination of) CVs provide a\ngood approximation of the RC for a given physical transformation, saddle\npoints in F(\\xi) correspond to the projection of the transition state\nensemble of configurations associated with that transformation. For\nconfigurations belonging to the transition state ensemble, the\nprobability of completing the crossing of the saddle point and\ncommitting to the products (see Sec. \n\nsec:committor)\nis narrowly distributed around \\frac{1}{2}.","type":"content","url":"/theory#reaction-coordinates","position":15},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"The Committor Function","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"type":"lvl3","url":"/theory#sec-committor","position":16},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl3":"The Committor Function","lvl2":"Collective Variables, Order Parameters, and Reaction Coordinates"},"content":"The committor function p_B(\\mathbf{r}), also known as splitting\nprobability, originally introduced by Onsager \n\nOnsager, 1938, provides the most rigorous definition of a reaction coordinate for a system evolving from state A to state B. It is the probability that a\ntrajectory starting from configuration \\mathbf{r}, with momenta drawn\nfrom equilibrium, reaches B before returning to A. By definition,\np_A(\\mathbf{r}) + p_B(\\mathbf{r}) = 1, and the transition state\nensemble corresponds to the isosurface p_B = 1/2. Iso-committor\nsurfaces partition configuration space into metastable basins, giving a\nunique dynamical measure of progress along the reaction. Unlike\nheuristic collective variables, the committor fully determines kinetic\nobservables such as rate constants and reactive fluxes (Vanden-Eijnden\n2006). Although computing it exactly is infeasible for high-dimensional\nsystems, it remains a key reference: an ideal reaction coordinate\ncorrelates monotonically with p_B(\\mathbf{r}) and minimizes its\nvariance within isosurfaces. Thus, the committor defines the optimal\nprojection of dynamics---any reduced representation that preserves its\ndistribution across the transition ensemble retains complete kinetic\ninformation \n\nGeissler et al., 1999, a\nconcept central to both path sampling \n\nBolhuis et al., 2002\n\nVan Erp & Bolhuis, 2005 and modern data-driven approaches.","type":"content","url":"/theory#sec-committor","position":17},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Free Energy Barriers and Generalised Transition State Theory"},"type":"lvl2","url":"/theory#free-energy-barriers-and-generalised-transition-state-theory","position":18},{"hierarchy":{"lvl1":"Theory: Defining a Free Energy Surface","lvl2":"Free Energy Barriers and Generalised Transition State Theory"},"content":"Once a suitable RC \\xi(\\mathbf{r}) is defined, the free energy surface\nF(\\xi) = -kT \\ln p(\\xi) quantifies the reversible work required to\nbring the system to a configuration of progress \\xi. Minima of\nF(\\xi) identify metastable states (reactants, products), while the\nmaximum along the minimum free-energy path defines the transition state\nat \\xi^\\ddagger. The corresponding free energy difference\\Delta F_\\xi^{\\ddagger} = F(\\xi^\\ddagger) - F(\\xi_\\mathrm{R})\n\nrepresents the free energy barrier that the system must overcome to\ntransform from reactants to products, thus opening the door to a kinetic\ninterpretation of the free energy surface. The generalised\ntransition-state theory (TST) links thermodynamics and kinetics by\nexpressing the rate constant as the thermal average of the flux through\na dividing surface in configuration space:k_{\\mathrm{TST}} =\n\\underbrace{\\frac{1}{2}\\langle|\\dot{\\xi}|\\rangle_{\\xi^\\ddagger}}_{\\text{kinetic prefactor}}\n\\underbrace{\\exp[-\\beta{\\Delta F_\\xi^{\\ddagger}}]}_{\\text{Boltzmann factor}}\n\nThe prefactor represents the average crossing\nvelocity, while the Boltzmann factor gives the equilibrium probability\nof reaching the transition state. This form follows from the\nflux--over--population formalism of Hänggi, Talkner, and\nBorkovec \n\nHänggi et al., 1990, where the rate of barrier\ncrossing is the ratio of the reactive flux J to the reactant\npopulation n_\\mathrm{R},k = \\frac{J}{n_\\mathrm{R}} = \\frac{\\int \\dot{\\xi}  \\delta(\\xi-\\xi^\\ddagger)  \\Theta(\\dot{\\xi})  e^{-\\beta H(\\mathbf{r},\\mathbf{p})}   d\\mathbf{r} d\\mathbf{p}}{\\int_{\\xi < \\xi^\\ddagger} e^{-\\beta H(\\mathbf{r},\\mathbf{p})}   d\\mathbf{r} d\\mathbf{p}}.\n\n Here \\delta(\\xi-\\xi^\\ddagger) selects\nconfigurations on the dividing surface, \\Theta(\\dot{\\xi}) retains\nforward crossings, and e^{-\\beta H} is the Boltzmann weight. Assuming\nequilibration of degrees of freedom orthogonal to \\xi,\nEq. \n\neq:TSTHanggi reduces to\nEq. \n\neq:TSTPeters, linking the rate constant directly to\nthe free-energy profile F(\\xi) obtainable with the methods discussed\nin Sec. \n\nsec:Computing. The exponential term is\nobtained from the simulation, while the prefactor follows from the mean\nthermal velocity along \\xi. The TST formula does not consider barrier\nrecrossing; these are fully included in approaches like the\nBennett-Chandler reactive flux(Bennett, n.d.; Chandler 1978)k_{BC}(t) = \\frac{\\langle \\dot{\\xi}(0)  \\delta[\\xi(0)-\\xi^\\ddagger]  h_B[\\xi(t)]\\rangle}\n{\\langle h_A \\rangle},\n\n where h_A and h_B are characteristic\nfunctions identifying reactants and products, and the rate k is\nobtained from the plateau value of k_{BC}(t). This formula and its\nsuccessive improvements \n\nRuiz-Montero et al., 1997 require an\naccurate sampling of the transition state and are limited by the\nnecessity to locate it and the assumption of its uniqueness. Where this\nis not true \n\nVanden-Eijnden, 2006\n\nE, Weinan & Vanden-Eijnden, 2010, the\nbundle of transition paths must be sampled, as discussed in Section\n\n\nsec:tps.","type":"content","url":"/theory#free-energy-barriers-and-generalised-transition-state-theory","position":19}]}