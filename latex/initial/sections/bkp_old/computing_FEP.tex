\subsection{FESs by Free Energy Perturbation}\

\subsubsection{Free Energy perturbation and thermodynamic integration}\TODO{reduce and make in a box? Yes, we can see in the end how everyting balances and cut where needed.}
The term Free Energy perturbation describes a series of methods that have their origin in the thermodynamic perturbation (TP) method, which evaluates the free-energy change for a jump from a reference Hamiltonian \(H_0\) to a perturbed one \(H_1\) via Zwanzig’s formula\cite{zwanzig1954high},
\[
\Delta G=-kT\ln \left\langle e^{-\beta(H_1-H_0)}\right\rangle_0,\label{eq:zwanzig}
\]
where the subscript 0 is a reminder that the average has to be taken with the unperturbed Hamiltonian \(H_0\). The formula is exact for any magnitude of the interaction energy \(H_1\), so the term ``perturbation'' is misleading. However, the sampling is efficient only when configurations sampled under \(H_0\) overlap well with those favored by \(H_1\). Stratifying the jump into neighboring states \(\lambda_i\to\lambda_{i+1}\) gives multistep TP,
\[
\Delta G=\sum_i -kT\ln\left\langle e^{-\beta[H(\lambda_{i+1})-H(\lambda_i)]}\right\rangle_{\lambda_i}.\label{eq:multiTP}
\] 

The very idea of TP is at the core of a large number of methods for the calculation of free energy differences and free energy profiles, which are described in the following sections.

\subsubsection{Umbrella Sampling}

Umbrella sampling \(US\) was introduced by Torrie and Valleau in the 1970s\cite{torrie1977nonphysical,kastner2011umbrella} as one of the earliest enhanced-sampling methods to compute free-energy differences. While it requires a degree of post-production effort to extract free energy profiles, it is a quite established method and is still frequently used, see for example the case study reported in Sec.\ref{sec:case-study-reduction}. The idea is to overcome the poor sampling of high-energy regions along a reaction coordinate \(\xi\) by applying a bias potential, most often harmonic, centered at a reference value \(\xi_i^{\mathrm{ref}}\):


\[
V_i(\xi) = \frac{1}{2} K (\xi - \xi_i^{\mathrm{ref}})^2 .
\]

A series of such biased simulations (``windows'') is performed to span the full range of \(\xi\). Each window produces a biased distribution \(P_i^b(\xi)\). The unbiased distribution is formally

\[
P_i^u(\xi) \;=\; P_i^b(\xi)\, e^{\beta V_i(\xi)}\,\langle e^{-\beta V_i(\xi)} \rangle^{-1},
\]
from which the free energy (or potential of mean force, PMF) is obtained as
\[
A(\xi) = -k_B T \ln P(\xi).
\]

Because each window only samples a narrow portion of \(\xi\), the problem of stitching the windows together arises: the free-energy offsets between windows \(F_i\) are not known directly.

Two major solutions exist. The most widely used is the Weighted Histogram Analysis Method (WHAM), developed by Kumar et al.\cite{kumar1992weighted}, which determines the offsets self-consistently by minimizing statistical error, thereby merging all window histograms into a single global distribution. WHAM has become the standard post-processing tool in umbrella sampling (see also the extension by Souaille and Roux\cite{souaille2001extension}) and there are efficient software packages for analysis and robust error estimate\cite{hub2010g_wham}.

An alternative to WHAM is umbrella integration, introduced by Kästner and Thiel\cite{kastner2005bridging}. Rather than reconstructing global histograms, this approach computes the mean force directly from the biased distribution:

\[
\frac{\partial A}{\partial \xi} \;=\; -kT\frac{\partial \ln P_i^b(\xi)}{\partial \xi} - \frac{dV_i}{d\xi}.
\]

For a harmonic bias, the additional term is simply \(K(\xi - \xi_i^{\mathrm{ref}})\). Integration of this mean force over \(\xi\) yields the PMF. In this way, umbrella integration makes explicit the proximity between US and thermodynamic integration methods.

Umbrella sampling is therefore best understood as a \emph{restrained sampling framework}, which can be analyzed either by histogram reweighting (WHAM) or by force-integration (umbrella integration). Both yield the same PMF given adequate sampling, but the distinction clarifies why US is often grouped with histogram-based estimators, while TI-type methods emphasize mean forces.

Over the years, umbrella sampling has inspired numerous extensions. Adaptive umbrella sampling\cite{mezei1987adaptive}iteratively builds the bias toward uniform sampling; local elevation umbrella sampling\cite{huber1994local,hansen2010using} introduces a history-dependent bias akin to early metadynamics\cite{laio2002escaping}, of which it can be considered a precursor. Multidimensional formulations like that of Bartels and Karplus\cite{bartels1997multidimensional} or K\"astner\cite{kastner2009umbrella}further allow simultaneous treatment of coupled coordinates. These methods have been hybridized with other techniques, leading to efficient self-learning adaptive variants\cite{wojtas2013self} or, as in the case of the combination of US with replica exchange techniques, versions that are amenable to extreme parallelization on supercomputers\cite{jiang2012calculation}, showing that US is more than ever a relevant algorithm in free energy landscape calculations.

\subsubsection{Thermodynamic Integration}

Thermodynamic integration (TI)  describes the same transformation by taking the limit of infinitesimally small changes, introducing a coupling parameter, and integrating the derivative along the path:
\[
\Delta G=\int_0^{1}\Big\langle \frac{\partial H}{\partial \lambda}\Big\rangle_{\lambda}d\lambda.\label{eq:TI}
\]
The TI formula eq.\ref{eq:TI} can be derived from eq. \ref{eq:multiTP} in the limit of infinitesimal changes \(\delta\lambda\) using the first term of the cumulant expansion\cite{kubo1962generalized} \( \left\langle e^{\epsilon \delta\lambda}\right\rangle \simeq \left\langle \epsilon \right\rangle \delta \lambda + O(\delta\lambda^2)\) of eq. \ref{eq:multiTP}, or by integrating \( \partial G/\partial \lambda  =  -(kT/Z) \partial Z/\partial \lambda\). 


This has, of course, the clear interpretation as the integral of the generalized mean force.  The historical ``slow-growth'' or single-configuration TI (SCTI)  replaces the ensemble average at each \(\lambda\) by a single instantaneous value taken while \(\lambda\) is being changed continuously using a prescribed protocol (e.g., \(\lambda(t) = \lambda(0) + t/t_\mathrm{max} [\lambda(1)-\lambda(0)]\).
While in the quasi-static (adiabatic) limit—infinitesimal \(\delta\lambda\) with full equilibration—SCTI collapses to standard TI, at finite switching rates \(d\lambda/dt= 1/t_\mathrm{max}\) the method shows hysteresis\cite{mitchell1991free}, a clear sign of non-equilibrium. Despite the development of more efficient TI methods, the flaw of implementing a non-equilibrium sampling has sparked new interest in SCTI in the context of Jarzynski's identity and steered molecular dynamics\cite{hummer2001fast,park2004calculating}
 --- see Section \ref{sec:single-molecule-spectroscopy} on single molecule spectroscopy.
Multiconfiguration TI (MCTI)\cite{straatsma1991multiconfiguration} is an improved variant of the TI family that calculates proper ensemble averages at each \(\lambda\) , yielding per-window statistical errors and letting allocate extra sampling exactly where fluctuations are largest and is, in fact, an embarrassingly parallel algorithm.

When the path steers an internal CV, the system has to be kept at or in the vicinity of the chosen value of \(\lambda\). A simple approach is to use a \(\lambda\)-dependent restraint \(U(q,\lambda)\) (typically in the form of a harmonic potential). In this case,  the free energy of the  unrestrained  system along that coordinate is recovered by correcting the TI  by a TP unbiasing\cite{straatsma1991multiconfiguration}:
\[
\Delta G(\lambda)=\int_{\lambda_0}^{\lambda}\left\langle \frac{\partial U}{\partial \lambda'}\right\rangle_{\lambda'}d\lambda'
+kT\ln\left\langle e^{\beta U(\lambda)}\right\rangle_{\lambda}
\]
By contrast,  purely alchemical TI  (modifying interaction parameters without biasing coordinates) needs no such correction. If holonomic constraints are imposed instead of restraints, correct unbiasing requires more complicated approaches, which have been discussed in depth in the development of the Blue Moon ensemble technique.

\subsubsection{Internal reaction coordinates and conditional probabilities}
Yet another way of looking at the TI when the RC is a function of atomic coordinates --explicitly, \(\xi(\mathbf r)\)-- is to write the PMF along the RC in terms of the conditional probability \(P_\xi(s)\)
\[w(s) = -kT \ln \left\langle \delta\left(\xi(\mathbf r) - s \right)\right\rangle \equiv - kT \ln P_\xi(s),
\]
The free energy difference takes the form 
\[ 
\Delta w = \int_{s_1}^{s_2} ds \left\langle \frac{\partial H}{\partial \xi}\right\rangle^\mathrm{cond}_{\xi=s}
\]
and the conditional average is defined by
\[\left\langle \,\cdot\,\right\rangle^\mathrm{cond}_{\xi=s}=\frac{\left\langle\,\cdot\,\delta\left( \xi -s \right)\right\rangle}{\left\langle \delta\left( \xi -s \right)\right\rangle }.
\]

\subsubsection{Handling constrained reaction coordinates}

Instead of using a restraint, one can use a holonomic constraint\cite{goldstein1950classical,van1984constraints} to enforce a specific value of the CV \(\xi(\mathbf {r})=s\) 
using algorithms like SHAKE\cite{ryckaert1977numerical}, where the constraining force  acts along \(\partial\xi/\partial \mathbf{r}\). In simple cases, the update of some of the atomic coordinates can simply be prevented. 



When computing the PMF from a dynamic with sucha a constraint, two geometric corrections appear when rewriting the conditional identity in terms of a coordinate-only average, namely an unbiasing factor 
\(Z_\xi= \sum_i \frac{1}{m_i}\left|\partial \xi/\partial \mathbf r_i\right|^2\)  that corrects for the loss of momentum along the consrtaint. The connection between conditional averages and (biased) averages in presence of a constraint is\cite{carter1989constrained}
\[
\left\langle X \right\rangle_{\xi=s}^\mathrm{cond} = \frac{\left\langle \sqrt{1/Z_\xi}  X \right\rangle}{\left\langle \sqrt{1/Z_\xi}  \right\rangle}_{\xi=s}
\]



and the full mass-metric tensor that appears when integrating over (all, including the unbiased) kinetic degrees of freedom.
The full Blue Moon ensemble configurational formula (so named because it helps sampling events that happens ``once in a blue moon'') is
\[
\frac{dW}{ds}
=\frac{\displaystyle \left\langle \sqrt{1/Z_\xi}\left[ \frac{\partial V}{\partial s}-\frac{k_BT}{2}\frac{\partial \ln|M(q)|}{\partial s}\right]\right\rangle_{\xi=s}}
{\displaystyle \left\langle \sqrt{1/Z_\xi}\right\rangle_{\xi=s}}.
\]
This formulation might be complicated to evaluate, in particular because the RC needs to be one of the generalized coordinates used to describe the configurations, and Sprick and Ciccotti, in a work that presents the whole Blue Moon ensemble in a very effective way\cite{sprik1998free}, derived an equivalent one that requires only the  constraint force magnitude  \(F_\xi\),
\[
\frac{dW}{ds}
=\frac{\displaystyle \left\langle \sqrt{1/Z_\xi}\left[ -F_\xi+kT G\_\xi \right]\right \rangle_{\xi=s}}{\displaystyle \left\langle \sqrt{1/Z_\xi}\right\rangle},
\]
along with a curvature correction
\[G_\xi=\frac{1}{Z_\xi^{2}}\sum_{i,j}\frac{1}{m_i m_j},
\frac{\partial \xi}{\partial \mathbf r_i}\frac{\partial^2 \xi}{\partial \mathbf r_i\partial \mathbf r_j}\frac{\partial \xi}{\partial \mathbf r_j}.
\]
In many common cases like that of a simple distance, \(\xi=r_{ij}\),\(Z_\xi\) is constant and \(G=0\), so the weights cancels out.

\begin{textbox}[h]\section{The trimer paradox and the geometric bias}

\begingroup
\setlength{\fboxsep}{0pt} % no vertical padding
\noindent
\newlength{\sidegut}\setlength{\sidegut}{1em} 
\hspace*{-\leftskip}\hspace*{\sidegut}%
\colorbox{white}{%
  \makebox[\dimexpr\linewidth+8\sidegut][c]{%
\includegraphics[width=.2\linewidth]{images/trimer.png}%
  }%
}%
\hspace*{\sidegut}\par
\endgroup

\noindent{}For a trimer of fixed bond lengths, one might naively expect the internal angle \(0\le\chi\le\pi\) to be uniformly distributed over the sphere of bond orientations, giving the density \(
P(\chi) = \frac{1}{2}\sin \chi\)
as in a random-walk picture. However, Kramers’ calculation\cite{kramers1946behavior} showed that the correct distribution carries an extra factor from the phase-space measure (the determinant of the metric tensor), 
\[
P(\chi) \propto \sin\chi\,\sqrt{1 - \frac{1}{4} \cos^2 \chi},
\]
which favors right-angled conformations. This ``bias'' arises because the moments of inertia, and hence the accessible momentum-space volume, depend on \(\chi\).
If one replaces the rigid rods with stiff harmonic springs (``Fraenkel springs'') and then takes the infinite-stiffness limit, the angle distribution reverts to the naive result \(P(\chi) \propto \sin\chi\), i.e., uniform on the sphere. So, an infinitely stiff bond is not the same as a rigid one! 
The resolution of this paradox\cite{van1984constraints} is that imposing holonomic constraints induces an entropic correction of geometric origin in the effective free energy,
\[
A_{\mathrm{geom}}(\chi) = -kT \ln \sqrt{\det M(\chi)},
\]
with \(M(\chi)\) the mass-metric tensor. If one wants to ``unbias'' constrained simulations back to the uniform-sphere distribution, this geometric term must be explicitly removed, and this term takes usually the name of Fixman potential\cite{fixman_classical_1974}.

\end{textbox}


\subsubsection{There's constraint and constraint}
It's worth discussing the difference between constraints introduced to enable the sampling, like that introduced in the Blue Moon ensemble,  but which would not be otherwise there under the conditions of a regular molecular dynamics simulation, and constraints like those used to keep molecular bonds rigid. In the latter case, unbiasing becomes borderline to a philosophical issue: if the constraint has been introduced to speed up an otherwise slow simulation because, unbiasing is welcome. A PMF can be, in some cases, unbiased using Fixman potential, although the correction is analytical only for fixed bond and angular constraints in non-branched molecules.\cite{fixman1978simulation}

Instead, if one believes that the rigid model is the best possible classical representation of, e.g., covalent bond lengths when the first excited state is way above the thermal energy, then no unbiasing procedure should be applied. While this is mostly correct, one should not forget that the zero point energy of a system depends on the conformation of the molecule, and so bond lengths cannot be strictly considered rigid\cite{van1984constraints}. Probably more importantly, these degrees of freedom, even if not strictly rigid, are certainly not obeying the equipartition theorem\cite{tolman1918general} and so unbiasing their kinetic contribution can introduce more problems than it solves.\TODO{momentum depending obsevables caveats}