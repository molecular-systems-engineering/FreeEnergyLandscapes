\subsection{Estimating \texorpdfstring{$p(\xi)$}{p(xi)}from Samples: Ergodicity} 
The ability to compute a FES, defined by Eq. \ref{eq:FES}, relies on the ability to calculate the marginal equilibrium probability distribution \(p(\xi)\). This probability distribution is formally obtained by marginalizing the canonical distribution; yet, in practice, it is estimated from trajectories generated by molecular dynamics or Monte Carlo simulations. The validity of such an estimate hinges on the assumption of \emph{ergodicity}: that the simulation explores the relevant regions of phase space in proportion to their equilibrium weights, so that time averages along the trajectory converge to ensemble averages. When ergodicity holds, the distribution of sampled configurations along $\xi$ is a faithful representation of the equilibrium probability distribution.
More rigorously, we define ergodicity as the property that time averages along a sufficiently long trajectory are equivalent to ensemble averages under the equilibrium distribution.
Formally, for an observable \(O(\mathbf{r})\):
\begin{equation}
\lim_{T \to \infty} \frac{1}{T} \int_0^T O(\mathbf{r}(t) dt = \int O(\mathbf{r} f(\mathbf{r}) d\mathbf{r}
\end{equation}
where \(f(\mathbf{r}) = \frac{e^{-\beta U(\mathbf{r})}}{Q_{NVT}}\) is the canonical distribution, $U(\mathbf{r})$ is the potential energy, and $Q_{NVT}$ the configurational partition function.
In the specific case of a collective variable $\xi(\mathbf{r})$, the ergodic hypothesis ensures that the empirical distribution of $\xi$ obtained from a trajectory, in the long-time limit ($T \to \infty$), converges to its equilibrium probability:
\begin{equation}
p(\xi) = \lim_{T \to \infty} \frac{1}{T} \int_0^T \delta\big(\xi(\mathbf{r}(t)) - \xi \big) dt = \frac{1}{Q_{NVT}} \int dr e^{-\beta U(\mathbf{r})} \delta\big(\xi(\mathbf{r}) - \xi\big).
\end{equation}

Difficulties arise when the system of interest is characterized by more than one metastable state, with high free energy barriers separating exhibits that exhibit rare events, slow collective motions, or multiple long-lived metastable states. In these cases, unbiased simulations on accessible timescales may fail to establish ergodic sampling, resulting in incomplete or biased estimates of $p(\xi)$. 
Addressing these limitations motivates the development of enhanced sampling methods, which artificially accelerate transitions and broaden sampling to restore ergodicity effectively. By ensuring that trajectories explore the full distribution of relevant states, these approaches allow accurate estimates of $p(\xi)$ and, consequently, reliable free energy surfaces that can be used to interpret thermodynamic stability and kinetic accessibility in complex molecular systems.

\input{sections/application_1_WaterDebenedetti} 

\subsection{Recovering Ergodicity via Biased Sampling: Foundations}
A class of approaches that enables recovering an ergodic sampling of all relevant states projected onto $\xi$ is based on the application of an additional, artificial potential \(V(\xi)\) that modifies the Hamiltonian of the system of interest. In the biased ensemble, configurations evolve under the modified Hamiltonian 
\begin{equation}
H_1 = H_0 + V(\xi)
\end{equation}
In all methods that follow this approach, \(V(\xi)\) is designed to enhance configurational sampling, by \emph{biasing} the equilibrium distribution $p(\xi)$. Under the effect of \(V(\xi)\), the distribution sampled is often indicated as a \emph{biased} distribution $p_b(\xi)$. The estimate of a FES from biased sampling, therefore, requires being able to recover the desired, \emph{unbiased} $p(\xi)$, from the sampled, \emph{biased}, $p_b(\xi)$. 
The process of computing $p(\xi)$ from $p_b(\xi)$ is commonly referred to as reweighting, and it is rooted in Zwanzig's free energy perturbation.
In the following, we introduce the foundational ideas of free energy perturbation and thermodynamic integration, then, by referring to these bases, we survey methods for the calculation of free energy surfaces from \emph{biased} Hamiltonians. In this section, after introducing foundational free energy estimators such as Free Energy Perturbation and Thermodynamic Integration, we include methods that build on \emph{static restraints} (Umbrella Sampling), \emph{holonomic constraints} (Blue Moon Ensemble), and \emph{adaptive bias} (Metadynamics and notable derived methods). The landscape of methods that enable the calculation of FESs is vast, encompassing a range of variants of the methods discussed here, as well as methods based on different principles. For an exhaustive survey on bias-based enhanced sampling methods for calculating FESs, the interested reader should refer to Ref. \cite{henin2022enhanced}. 

\subsubsection{Free Energy Perturbation and Thermodynamic Integration}\label{sec:FEP}
The term Free Energy perturbation (FEP) describes a series of methods that have their origin in the thermodynamic perturbation (TP) method, which evaluates the free-energy change for a jump from a reference Hamiltonian \(H_0\) to a perturbed one \(H_1\) via Zwanzig’s formula \cite{zwanzig1954high},
\begin{equation}
\Delta F = -kT\ln \left\langle e^{-\beta(H_1-H_0)}\right\rangle_0,\label{eq:zwanzig}
\end{equation}
where the subscript 0 is a reminder that the average has to be taken with the unperturbed Hamiltonian \(H_0\). The formula is exact for any magnitude of the interaction energy \(H_1\), so the term ``perturbation'' is somewhat misleading. However, the sampling is efficient only when configurations sampled under \(H_0\) overlap well with those favored by \(H_1\). Stratifying the jump into neighboring states \(\lambda_i\to\lambda_{i+1}\) gives multistep TP,
\begin{equation}
\Delta F=\sum_i -kT\ln\left\langle e^{-\beta[H(\lambda_{i+1})-H(\lambda_i)]}\right\rangle_{\lambda_i}.\label{eq:multiTP}
\end{equation}
The very idea of TP is at the core of a large number of methods for the calculation of free energy differences and free energy profiles, which are described in the following sections.

\subsubsection{Thermodynamic Integration}
Thermodynamic integration (TI) describes the transformation between two Hamiltonians, taking the limit of infinitesimally small changes, obtained by introducing a coupling parameter, and integrating the derivative along the path:
\begin{equation}
\Delta F=\int_0^{1}\Big\langle \frac{\partial H}{\partial \lambda}\Big\rangle_{\lambda}d\lambda.\label{eq:TI}
\end{equation}
The TI formula eq.\ref{eq:TI} can be derived from eq. \ref{eq:multiTP} in the limit of infinitesimal changes \(\delta\lambda\) using the first term of the cumulant expansion \cite{kubo1962generalized} \( \left\langle e^{\epsilon \delta\lambda}\right\rangle \simeq \left\langle \epsilon \right\rangle \delta \lambda + O(\delta\lambda^2)\) of eq. \ref{eq:multiTP}, or by integrating \( \partial F/\partial \lambda  =  -(kT/Z) \partial Z/\partial \lambda\). 

This, of course, has the precise interpretation as the integral of the generalized mean force.  The historical ``slow-growth'' or single-configuration TI (SCTI)  replaces the ensemble average at each \(\lambda\) by a single instantaneous value taken while \(\lambda\) is being changed continuously using a prescribed protocol (e.g., \(\lambda(t) = \lambda(0) + t/t_\mathrm{max} [\lambda(1)-\lambda(0)]\).
While in the quasi-static (adiabatic) limit—infinitesimal \(\delta\lambda\) with full equilibration—SCTI collapses to standard TI, at finite switching rates \(d\lambda/dt= 1/t_\mathrm{max}\) the method shows hysteresis \cite{mitchell1991free}, a clear sign of non-equilibrium. Despite the development of more efficient TI methods, the flaw of implementing a non-equilibrium sampling has sparked new interest in SCTI in the context of Jarzynski's identity and steered molecular dynamics \cite{hummer2001fast,park2004calculating}
 --- see Section \ref{sec:single-molecule-spectroscopy} on single molecule spectroscopy.
Multiconfiguration TI (MCTI) \cite{straatsma1991multiconfiguration} is an improved variant of the TI family that calculates proper ensemble averages at each \(\lambda\), yielding per-window statistical errors and allowing for extra sampling to be allocated exactly where fluctuations are largest. It is, in fact, an embarrassingly parallel algorithm.

When the path steers an internal CV, the system has to be kept at or in the vicinity of the chosen value of \(\lambda\). A simple approach is to use a \(\lambda\)-dependent restraint \(U(q,\lambda)\) (typically in the form of a harmonic potential). In this case,  the free energy of the  unrestrained  system along that coordinate is recovered by correcting the TI  by a TP unbiasing \cite{straatsma1991multiconfiguration}:
\begin{equation}
\Delta F(\lambda)=\int_{\lambda_0}^{\lambda}\left\langle \frac{\partial U}{\partial \lambda'}\right\rangle_{\lambda'}d\lambda'
+kT\ln\left\langle e^{\beta U(\lambda)}\right\rangle_{\lambda}
\label{eq:TI_gen}
\end{equation}
By contrast, purely alchemical TI (modifying interaction parameters without biasing coordinates) requires no such correction. 

%\paragraph{Internal reaction coordinates and conditional probabilities}

Yet another way of looking at the TI when the RC is a function of atomic coordinates instead of a parameter of the Hamiltonian, for example 
$\xi(\mathbf{r}) = \left| \mathbf{r}_1 - \mathbf{r}_1 \right|$,
is to write the PMF along the RC in terms of the conditional probability $p_{\xi}(s)$
\begin{equation}
w(s) = -kT \ln \left\langle \delta\left(\xi(\mathbf r) - s \right)\right\rangle \equiv - kT \ln P_\xi(s),
\end{equation}
The free energy difference takes the form 
\begin{equation}
\Delta w = \int_{s_1}^{s_2} ds \left\langle \frac{\partial H}{\partial \xi}\right\rangle^\mathrm{cond}_{\xi=s}
\end{equation}
and the conditional average is defined by
\begin{equation}\left\langle \,\cdot\,\right\rangle^\mathrm{cond}_{\xi=s}=\frac{\left\langle\,\cdot\,\delta\left( \xi -s \right)\right\rangle}{\left\langle \delta\left( \xi -s \right)\right\rangle}.\end{equation}

If holonomic constraints are imposed instead of restraints, correct unbiasing requires more complicated approaches, which have been discussed in depth in the development of the Blue Moon ensemble technique.

\begin{figure}
\includegraphics[width=0.8\linewidth,trim=0 0 0 0,clip]{Figures/Figure_biased_sampling.png} 
\caption{Illustration of biased sampling strategies for reconstructing free energy surfaces along a collective variable, $\xi$. Panels (a,b) show unbiased sampling, where transitions between metastable states are hindered by large free energy barriers, resulting in poor sampling of the high-energy region and estimates of $p(\xi)$ are limited to a localised region, which depends on initial conditions. Panels (c,d) illustrate umbrella sampling, in which multiple restrained simulations centered at reference positions $\xi_{ref}$ (red parabolas) enabling uniform coverage of the reaction coordinate as shown in panel d; the resulting biased histograms are subsequently combined, for example through the Weighted Histogram Analysis Method (WHAM) or Umbrella Integration, to recover the full free energy profile. Panels (e,f) depict the time evolution of metadynamics, where (e) shows the evolution of the history-dependent bias, progressively filling free energy wells and promoting barrier crossing. In (f), the dynamics of $\xi(t)$ are reported, showing how the ergodic exploration of configuration space is progressively achieved.}
\label{fig:biased}
\end{figure}

\subsection{Umbrella Sampling}\label{sec:UmbrellaSampling}
Umbrella sampling (US) was introduced by Torrie and Valleau in the 1970s \cite{torrie1977nonphysical,kastner2011umbrella} as one of the earliest enhanced-sampling methods to compute a FES along a CV \(\xi\). US is a \emph{static} bias method, i.e., the potential introduced as a perturbation to the system's Hamiltonian is only a function of \(\xi\), and does not change in time. The central idea of US is to overcome the poor sampling of high-energy regions in \(\xi\) by introducing a - usually harmonic - restraint, that localizes the sampling around a reference value $\xi_i^{\mathrm{ref}}$:
\begin{equation}
V_i(\xi) = \frac{1}{2} K (\xi - \xi_i^{\mathrm{ref}})^2 .
\end{equation}
In the context of TP, discussed in the previous subsection, $V_i(\xi)$ can be seen as the \emph{perturbation} introduced in the physical Hamiltonian of the system in each of the $i$ simulations. 

In US, a series of such biased simulations (``windows'', Fig. \ref{fig:biased}b ) is performed to span the full range of \(\xi\) (see Fig. \ref{fig:biased}e). Each window produces a biased distribution \(p_i^b(\xi)\). The unbiased distribution in window $i$ is formally recovered as
\begin{equation}
p_i(\xi) \;=\; p_i^b(\xi)\, e^{\beta V_i(\xi)}\,\langle e^{-\beta V_i(\xi)} \rangle^{-1},
\end{equation}
from which the free energy (or potential of mean force, PMF) is obtained as using Eq. \ref{eq:FES}. Because each window only samples a narrow portion of \(\xi\), the problem of stitching the windows together arises: the free-energy offsets between windows \(F_i\) are not known directly.

Several solutions to this problem exist, which build on FEP and TI concepts \cite{roux1995calculation}. The most widely used is the Weighted Histogram Analysis Method (WHAM), developed by Kumar et al. \cite{kumar1992weighted}, which determines the offsets self-consistently by minimizing statistical error, thereby merging all window histograms into a single global distribution. WHAM has become the standard post-processing tool in umbrella sampling (see also the extension by Souaille and Roux \cite{souaille2001extension}), and there are efficient software packages for analysis and robust error estimate \cite{hub2010g_wham}. The more recent Multistate Bennet Acceptance Ratio approach provides an alternative method to estimate the set of offset constants $F_i$, which can be shown to be theoretically equivalent to binless WHAM. \cite{shirts2008statistically,tan2012theory}

An alternative to WHAM and MBAR that side-steps completely the need to estimate offset constants $F_i$ is umbrella integration (UI), introduced by Kästner and Thiel \cite{kastner2005bridging}. Rather than reconstructing global histograms, this approach computes the mean force directly from the biased distribution:
\begin{equation}
\frac{\partial F}{\partial \xi} \;=\; -kT\frac{\partial \ln p_i^b(\xi)}{\partial \xi} - \frac{dV_i}{d\xi}.
\end{equation}
For a harmonic bias, the additional term is \(K(\xi - \xi_i^{\mathrm{ref}})\). Integration of this mean force over \(\xi\) yields the PMF. In this way, umbrella integration makes explicit the proximity between US and thermodynamic integration methods, effectively implementing a FES estimator similar to Eq. \ref{eq:TI_gen}

Umbrella sampling is therefore best understood as a \emph{restrained sampling framework}, which can be analyzed either by histogram reweighting (WHAM, MBAR) or by force integration (UI). Both yield the same PMF given adequate sampling, but the distinction clarifies why US is often grouped with histogram-based estimators, while TI-type methods emphasize mean forces.


\input{sections/application_4_FEP_catalysis_US}

Over the years, umbrella sampling has inspired numerous extensions. Adaptive umbrella sampling \cite{mezei1987adaptive}iteratively builds the bias toward uniform sampling; local elevation umbrella sampling \cite{huber1994local,hansen2010using} introduces a history-dependent bias akin to early metadynamics \cite{laio2002escaping}, of which it can be considered a precursor. Multidimensional formulations like that of Bartels and Karplus \cite{bartels1997multidimensional} or K\"astner \cite{kastner2009umbrella}further allow simultaneous treatment of coupled coordinates. These methods have been hybridized with other techniques, leading to efficient self-learning adaptive variants \cite{wojtas2013self} or, as in the case of the combination of US with replica exchange techniques, versions that are amenable to extreme parallelization on supercomputers \cite{jiang2012calculation}, showing that US is more than ever a relevant algorithm in free energy landscape calculations.



\subsection{Constrained reaction coordinates: the Blue Moon Ensemble}
Instead of using a Harmonic restraint as in US, one can use instead holonomic constraints \cite{goldstein1950classical,van1984constraints} to enforce a specific value of the CV \(\xi(\mathbf {r})=s\) 
using algorithms like SHAKE \cite{ryckaert1977numerical}, where the constraining force  acts along \(\partial\xi/\partial \mathbf{r}\). In simple cases, the update of some atomic coordinates can be prevented. 

When computing the PMF from a dynamic with such a constraint, two geometric corrections appear in the conditional identity in terms of a coordinate-only average, namely an unbiasing factor 
\(Z_{\xi}= \sum_i \frac{1}{m_i}\left|\partial {\xi}/\partial \mathbf r_i\right|^2\)  that corrects for the loss of momentum along the constraint. The connection between conditional averages and (biased) averages in the presence of a constraint is \cite{carter1989constrained}
\begin{equation}
\left\langle X \right\rangle_{\xi=s}^\mathrm{cond} = \frac{\left\langle \sqrt{1/Z_\xi}  X \right\rangle_{\xi=s}}{\left\langle \sqrt{1/Z_\xi}  \right\rangle}_{\xi=s}
\end{equation}
and the full mass-metric tensor that appears when integrating over (all, including the unbiased) kinetic degrees of freedom.
The full Blue Moon ensemble configurational formula (so named because it helps sample events that happen ``once in a blue moon'') is
\begin{equation}
\frac{dW}{ds}
=\frac{
\displaystyle \left\langle \sqrt{1/Z_\xi}\left[ \frac{\partial V}{\partial s}-\frac{kT}{2}\frac{\partial \ln|M(q)|}{\partial s}\right]\right\rangle_{\xi=s}}
{\displaystyle \left\langle \sqrt{1/Z_\xi}\right\rangle_{\xi=s}}.
\end{equation}
This formulation might be complicated to evaluate, in particular because the RC needs to be one of the generalized coordinates used to describe the configurations, and Sprick and Ciccotti, in a work that presents the whole Blue Moon ensemble in a very effective way \cite{sprik1998free}, derived an equivalent one that requires only the  constraint force magnitude  \(F_\xi\),
\begin{equation}
\frac{dW}{ds}
=\frac{\displaystyle \left\langle \sqrt{1/Z_\xi}\left[ -F_\xi+kT G_\xi \right]\right \rangle_{\xi=s}}{\displaystyle \left\langle \sqrt{1/Z_\xi}\right\rangle},
\end{equation}
along with a curvature correction
\begin{equation}
G_\xi=\frac{1}{Z_\xi^{2}}{\sum_{i,j}\frac{1}{m_i m_j}
\frac{\partial \xi}{\partial \mathbf r_i}\frac{\partial^2 \xi}{\partial \mathbf r_i\partial \mathbf r_j}\frac{\partial \xi}{\partial \mathbf r_j}}
\end{equation}
In many common cases like that of a simple distance, \(\xi=r_{ij}\),\(Z_\xi\) is constant and \(G=0\), so the weights cancel out. The case of multidimensional reaction constants is discussed in Ref.\cite{ciccotti1991molecular}.


\begin{textbox}[t]\section{The trimer paradox and the geometric bias}

\begingroup
\setlength{\fboxsep}{0pt} % no vertical padding
\noindent
\newlength{\sidegut}\setlength{\sidegut}{1em} 
\hspace*{-\leftskip}\hspace*{\sidegut}%
\colorbox{white}{%
  \makebox[\dimexpr\linewidth+8\sidegut][c]{%
\includegraphics[width=.2\linewidth]{images/trimer.png}%
  }%
}%
\hspace*{\sidegut}\par
\endgroup

\noindent{}For a trimer of fixed bond lengths, one might naively expect the internal angle \(0\le\chi\le\pi\) to be uniformly distributed over the sphere of bond orientations, giving the density \(
P(\chi) = \frac{1}{2}\sin \chi\)
as in a random-walk picture. However, Kramers’ calculation \cite{kramers1946behavior} showed that the correct distribution carries an extra factor from the phase-space measure (the determinant of the metric tensor), 
\[
P(\chi) \propto \sin\chi\,\sqrt{1 - \frac{1}{4} \cos^2 \chi},
\]
which favors right-angled conformations. This ``bias'' arises because the moments of inertia, and hence the accessible momentum-space volume, depend on \(\chi\).
If one replaces the rigid rods with stiff harmonic springs (``Fraenkel springs'') and then takes the infinite-stiffness limit, the angle distribution reverts to the naive result \(P(\chi) \propto \sin\chi\), i.e., uniform on the sphere. So, an infinitely stiff bond is not the same as a rigid one! 
The resolution of this paradox \cite{van1984constraints} is that imposing holonomic constraints induces an entropic correction of geometric origin in the effective free energy,
\[
A_{\mathrm{geom}}(\chi) = -kT \ln \sqrt{\det M(\chi)},
\]
with \(M(\chi)\) the mass-metric tensor. Suppose one wants to ``unbias'' constrained simulations back to the uniform-sphere distribution. In that case, this geometric term must be explicitly removed, and this term usually takes the name of Fixman potential \cite{fixman_classical_1974}. 
\end{textbox}

It's essential to note that the approach to treating a constraint depends on the reason it was introduced. For example, the constraints used in the Blue Moon are an artifact of the method, and their biasing effect must be removed. The bias of constraints that keep molecular structures rigid (typically, bonds or angles in empirical force field simulations) can be removed, at least in some cases, via the Fixman potential \cite{fixman1978simulation}. Whether removing this bias is the correct thing to do or not is an open point, as, in general, both flexible and constrained bonds are approximations. The former do not take into account that excited vibrational states are often way above the thermal energy. In contrast, the second disregards that the zero-point energy of the system, and thus the bond lengths, depend on the molecular conformation \cite{van1984constraints}. Even if not strictly rigid, however, these degrees of freedom do not obey the equipartition theorem \cite{tolman1918general} and thus unbiasing them is unlikely to be the correct approach.



 
