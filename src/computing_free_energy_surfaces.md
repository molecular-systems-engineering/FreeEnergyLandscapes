# Computing Free Energy Surfaces {#sec:Computing}

## Estimating $p(\xi)$from Samples: Ergodicity

The ability to compute a FES, defined by Eq. [eq:FES](#eq:FES), relies
on the ability to calculate the marginal equilibrium probability
distribution $p(\xi)$. This probability distribution is formally
obtained by marginalizing the canonical distribution; yet, in practice,
it is estimated from trajectories generated by molecular dynamics or
Monte Carlo simulations. The validity of such an estimate hinges on the
assumption of *ergodicity*: that the simulation explores the relevant
regions of phase space in proportion to their equilibrium weights, so
that time averages along the trajectory converge to ensemble averages.
When ergodicity holds, the distribution of sampled configurations along
$\xi$ is a faithful representation of the equilibrium probability
distribution. More rigorously, we define ergodicity as the property that
time averages along a sufficiently long trajectory are equivalent to
ensemble averages under the equilibrium distribution. Formally, for an
observable $O(\mathbf{r})$:
$$\lim_{T \to \infty} \frac{1}{T} \int_0^T O(\mathbf{r}(t) dt = \int O(\mathbf{r} f(\mathbf{r}) d\mathbf{r}$$
where $f(\mathbf{r}) = \frac{e^{-\beta U(\mathbf{r})}}{Q_{NVT}}$ is the
canonical distribution, $U(\mathbf{r})$ is the potential energy, and
$Q_{NVT}$ the configurational partition function. In the specific case
of a collective variable $\xi(\mathbf{r})$, the ergodic hypothesis
ensures that the empirical distribution of $\xi$ obtained from a
trajectory, in the long-time limit ($T \to \infty$), converges to its
equilibrium probability:
$$p(\xi) = \lim_{T \to \infty} \frac{1}{T} \int_0^T \delta\big(\xi(\mathbf{r}(t)) - \xi \big) dt = \frac{1}{Q_{NVT}} \int dr e^{-\beta U(\mathbf{r})} \delta\big(\xi(\mathbf{r}) - \xi\big).$$

Difficulties arise when the system of interest is characterized by more
than one metastable state, with high free energy barriers separating
exhibits that exhibit rare events, slow collective motions, or multiple
long-lived metastable states. In these cases, unbiased simulations on
accessible timescales may fail to establish ergodic sampling, resulting
in incomplete or biased estimates of $p(\xi)$. Addressing these
limitations motivates the development of enhanced sampling methods,
which artificially accelerate transitions and broaden sampling to
restore ergodicity effectively. By ensuring that trajectories explore
the full distribution of relevant states, these approaches allow
accurate estimates of $p(\xi)$ and, consequently, reliable free energy
surfaces that can be used to interpret thermodynamic stability and
kinetic accessibility in complex molecular systems.

:::{important}
:icon: false
### Discovering and Mapping Elusive Metastable States.[]{#sec:case-study-water label="sec:case-study-water"}

A particularly enlightening application of enhanced sampling methods in
the exploration of metastable states has concerned the debate revolving
around the existence of a first order phase transition (and an
associated critical point) between two putative forms of liquid water at
low temperature and high
pressure[@poole1992phase; @palmer2014metastable; @gallo2016water] The
two states, low density liquid (LDL) and high density liquid (HDL) have
a clearly different density, but density alone is not enough to clearly
resolve the free energy landscape, as it is not able to distinguish LDL
water from incipient crystallization into the nearest ice polymorph,
cubic ice. In their landmark study, Palmer and colleagues, using six
different sampling techniques (umbrella sampling MC, well-tempered
metadynamics, unconstrained MC, hybrid MC, parallel tempering MC, and
Hamiltonian exchange MC simulations) for robustness, showed that the two
liquid forms are separated by a $4 kT$ barrier, with finite-size scaling
characteristic of first-order transitions. Both liquid states are
metastable with respect to the cubic form of ice.

![image](images/debenedetti.pdf){width="\\linewidth"} *Free energy
landscape of the ST2 water model at 228.6 K and 2.4 kbar using density
$\rho$ and the crystalline order parameter, $Q_6$[@steinhardt1983bond],
as collective variable. The HDL and LDL basins, characterised by a low
order ($Q_6\simeq 0.05)$ are separated by a barrier of about $4 kT$.
Both liquid states are metastable with respect to cubic ice
($Q_6\simeq 0.52)$. Contour lines are separated by $1 kT$
[@palmer2014metastable]*

:::

## Recovering Ergodicity via Biased Sampling: Foundations

A class of approaches that enables recovering an ergodic sampling of all
relevant states projected onto $\xi$ is based on the application of an
additional, artificial potential $V(\xi)$ that modifies the Hamiltonian
of the system of interest. In the biased ensemble, configurations evolve
under the modified Hamiltonian $$H_1 = H_0 + V(\xi)$$ In all methods
that follow this approach, $V(\xi)$ is designed to enhance
configurational sampling, by *biasing* the equilibrium distribution
$p(\xi)$. Under the effect of $V(\xi)$, the distribution sampled is
often indicated as a *biased* distribution $p_b(\xi)$. The estimate of a
FES from biased sampling, therefore, requires being able to recover the
desired, *unbiased* $p(\xi)$, from the sampled, *biased*, $p_b(\xi)$.
The process of computing $p(\xi)$ from $p_b(\xi)$ is commonly referred
to as reweighting, and it is rooted in Zwanzig's free energy
perturbation. In the following, we introduce the foundational ideas of
free energy perturbation and thermodynamic integration, then, by
referring to these bases, we survey methods for the calculation of free
energy surfaces from *biased* Hamiltonians. In this section, after
introducing foundational free energy estimators such as Free Energy
Perturbation and Thermodynamic Integration, we include methods that
build on *static restraints* (Umbrella Sampling), *holonomic
constraints* (Blue Moon Ensemble), and *adaptive bias* (Metadynamics and
notable derived methods). The landscape of methods that enable the
calculation of FESs is vast, encompassing a range of variants of the
methods discussed here, as well as methods based on different
principles. For an exhaustive survey on bias-based enhanced sampling
methods for calculating FESs, the interested reader should refer to Ref.
[@henin2022enhanced].

### Free Energy Perturbation and Thermodynamic Integration[]{#sec:FEP label="sec:FEP"}

The term Free Energy perturbation (FEP) describes a series of methods
that have their origin in the thermodynamic perturbation (TP) method,
which evaluates the free-energy change for a jump from a reference
Hamiltonian $H_0$ to a perturbed one $H_1$ via Zwanzig's formula
[@zwanzig1954high],
$$\Delta F = -kT\ln \left\langle e^{-\beta(H_1-H_0)}\right\rangle_0,\label{eq:zwanzig}$$
where the subscript 0 is a reminder that the average has to be taken
with the unperturbed Hamiltonian $H_0$. The formula is exact for any
magnitude of the interaction energy $H_1$, so the term "perturbation" is
somewhat misleading. However, the sampling is efficient only when
configurations sampled under $H_0$ overlap well with those favored by
$H_1$. Stratifying the jump into neighboring states
$\lambda_i\to\lambda_{i+1}$ gives multistep TP,
$$\Delta F=\sum_i -kT\ln\left\langle e^{-\beta[H(\lambda_{i+1})-H(\lambda_i)]}\right\rangle_{\lambda_i}.\label{eq:multiTP}$$
The very idea of TP is at the core of a large number of methods for the
calculation of free energy differences and free energy profiles, which
are described in the following sections.

### Thermodynamic Integration

Thermodynamic integration (TI) describes the transformation between two
Hamiltonians, taking the limit of infinitesimally small changes,
obtained by introducing a coupling parameter, and integrating the
derivative along the path:
$$\Delta F=\int_0^{1}\Big\langle \frac{\partial H}{\partial \lambda}\Big\rangle_{\lambda}d\lambda.\label{eq:TI}$$
The TI formula eq.[eq:TI](#eq:TI) can be derived from eq.
[eq:multiTP](#eq:multiTP) in the limit of infinitesimal changes
$\delta\lambda$ using the first term of the cumulant expansion
[@kubo1962generalized]
$\left\langle e^{\epsilon \delta\lambda}\right\rangle \simeq \left\langle \epsilon \right\rangle \delta \lambda + O(\delta\lambda^2)$
of eq. [eq:multiTP](#eq:multiTP), or by integrating
$\partial F/\partial \lambda  =  -(kT/Z) \partial Z/\partial \lambda$.

This, of course, has the precise interpretation as the integral of the
generalized mean force. The historical "slow-growth" or
single-configuration TI (SCTI) replaces the ensemble average at each
$\lambda$ by a single instantaneous value taken while $\lambda$ is being
changed continuously using a prescribed protocol (e.g.,
$\lambda(t) = \lambda(0) + t/t_\mathrm{max} [\lambda(1)-\lambda(0)]$.
While in the quasi-static (adiabatic) limit---infinitesimal
$\delta\lambda$ with full equilibration---SCTI collapses to standard TI,
at finite switching rates $d\lambda/dt= 1/t_\mathrm{max}$ the method
shows hysteresis [@mitchell1991free], a clear sign of non-equilibrium.
Despite the development of more efficient TI methods, the flaw of
implementing a non-equilibrium sampling has sparked new interest in SCTI
in the context of Jarzynski's identity and steered molecular dynamics
[@hummer2001fast; @park2004calculating] --- see Section
[sec:single-molecule-spectroscopy](#sec:single-molecule-spectroscopy) on
single molecule spectroscopy. Multiconfiguration TI (MCTI)
[@straatsma1991multiconfiguration] is an improved variant of the TI
family that calculates proper ensemble averages at each $\lambda$,
yielding per-window statistical errors and allowing for extra sampling
to be allocated exactly where fluctuations are largest. It is, in fact,
an embarrassingly parallel algorithm.

When the path steers an internal CV, the system has to be kept at or in
the vicinity of the chosen value of $\lambda$. A simple approach is to
use a $\lambda$-dependent restraint $U(q,\lambda)$ (typically in the
form of a harmonic potential). In this case, the free energy of the
unrestrained system along that coordinate is recovered by correcting the
TI by a TP unbiasing [@straatsma1991multiconfiguration]:
$$\Delta F(\lambda)=\int_{\lambda_0}^{\lambda}\left\langle \frac{\partial U}{\partial \lambda'}\right\rangle_{\lambda'}d\lambda'
+kT\ln\left\langle e^{\beta U(\lambda)}\right\rangle_{\lambda}
\label{eq:TI_gen}$$ By contrast, purely alchemical TI (modifying
interaction parameters without biasing coordinates) requires no such
correction.

Yet another way of looking at the TI when the RC is a function of atomic
coordinates instead of a parameter of the Hamiltonian, for example
$\xi(\mathbf{r}) = \left| \mathbf{r}_1 - \mathbf{r}_1 \right|$, is to
write the PMF along the RC in terms of the conditional probability
$p_{\xi}(s)$
$$w(s) = -kT \ln \left\langle \delta\left(\xi(\mathbf r) - s \right)\right\rangle \equiv - kT \ln P_\xi(s),$$
The free energy difference takes the form
$$\Delta w = \int_{s_1}^{s_2} ds \left\langle \frac{\partial H}{\partial \xi}\right\rangle^\mathrm{cond}_{\xi=s}$$
and the conditional average is defined by
$$\left\langle \,\cdot\,\right\rangle^\mathrm{cond}_{\xi=s}=\frac{\left\langle\,\cdot\,\delta\left( \xi -s \right)\right\rangle}{\left\langle \delta\left( \xi -s \right)\right\rangle}.$$

If holonomic constraints are imposed instead of restraints, correct
unbiasing requires more complicated approaches, which have been
discussed in depth in the development of the Blue Moon ensemble
technique.

```{figure} Figures/Figure_biased_sampling.png
:name: fig:biased
:width: 0.8\linewidth
Illustration of biased sampling strategies for reconstructing free energy surfaces along a collective variable, $\xi$. Panels (a,b) show unbiased sampling, where transitions between metastable states are hindered by large free energy barriers, resulting in poor sampling of the high-energy region and estimates of $p(\xi)$ are limited to a localised region, which depends on initial conditions. Panels (c,d) illustrate umbrella sampling, in which multiple restrained simulations centered at reference positions $\xi_{ref}$ (red parabolas) enabling uniform coverage of the reaction coordinate as shown in panel d; the resulting biased histograms are subsequently combined, for example through the Weighted Histogram Analysis Method (WHAM) or Umbrella Integration, to recover the full free energy profile. Panels (e,f) depict the time evolution of metadynamics, where (e) shows the evolution of the history-dependent bias, progressively filling free energy wells and promoting barrier crossing. In (f), the dynamics of $\xi(t)$ are reported, showing how the ergodic exploration of configuration space is progressively achieved.
```

## Umbrella Sampling[]{#sec:UmbrellaSampling label="sec:UmbrellaSampling"}

Umbrella sampling (US) was introduced by Torrie and Valleau in the 1970s
[@torrie1977nonphysical; @kastner2011umbrella] as one of the earliest
enhanced-sampling methods to compute a FES along a CV $\xi$. US is a
*static* bias method, i.e., the potential introduced as a perturbation
to the system's Hamiltonian is only a function of $\xi$, and does not
change in time. The central idea of US is to overcome the poor sampling
of high-energy regions in $\xi$ by introducing a - usually harmonic -
restraint, that localizes the sampling around a reference value
$\xi_i^{\mathrm{ref}}$:
$$V_i(\xi) = \frac{1}{2} K (\xi - \xi_i^{\mathrm{ref}})^2 .$$ In the
context of TP, discussed in the previous subsection, $V_i(\xi)$ can be
seen as the *perturbation* introduced in the physical Hamiltonian of the
system in each of the $i$ simulations.

In US, a series of such biased simulations ("windows", Fig.
[fig:biased](#fig:biased)b ) is performed to span the full range of
$\xi$ (see Fig. [fig:biased](#fig:biased)e). Each window produces a
biased distribution $p_i^b(\xi)$. The unbiased distribution in window
$i$ is formally recovered as
$$p_i(\xi) \;=\; p_i^b(\xi)\, e^{\beta V_i(\xi)}\,\langle e^{-\beta V_i(\xi)} \rangle^{-1},$$
from which the free energy (or potential of mean force, PMF) is obtained
as using Eq. [eq:FES](#eq:FES). Because each window only samples a
narrow portion of $\xi$, the problem of stitching the windows together
arises: the free-energy offsets between windows $F_i$ are not known
directly.

Several solutions to this problem exist, which build on FEP and TI
concepts [@roux1995calculation]. The most widely used is the Weighted
Histogram Analysis Method (WHAM), developed by Kumar et al.
[@kumar1992weighted], which determines the offsets self-consistently by
minimizing statistical error, thereby merging all window histograms into
a single global distribution. WHAM has become the standard
post-processing tool in umbrella sampling (see also the extension by
Souaille and Roux [@souaille2001extension]), and there are efficient
software packages for analysis and robust error estimate
[@hub2010g_wham]. The more recent Multistate Bennet Acceptance Ratio
approach provides an alternative method to estimate the set of offset
constants $F_i$, which can be shown to be theoretically equivalent to
binless WHAM. [@shirts2008statistically; @tan2012theory]

An alternative to WHAM and MBAR that side-steps completely the need to
estimate offset constants $F_i$ is umbrella integration (UI), introduced
by Kästner and Thiel [@kastner2005bridging]. Rather than reconstructing
global histograms, this approach computes the mean force directly from
the biased distribution:
$$\frac{\partial F}{\partial \xi} \;=\; -kT\frac{\partial \ln p_i^b(\xi)}{\partial \xi} - \frac{dV_i}{d\xi}.$$
For a harmonic bias, the additional term is
$K(\xi - \xi_i^{\mathrm{ref}})$. Integration of this mean force over
$\xi$ yields the PMF. In this way, umbrella integration makes explicit
the proximity between US and thermodynamic integration methods,
effectively implementing a FES estimator similar to Eq.
[eq:TI_gen](#eq:TI_gen)

Umbrella sampling is therefore best understood as a *restrained sampling
framework*, which can be analyzed either by histogram reweighting (WHAM,
MBAR) or by force integration (UI). Both yield the same PMF given
adequate sampling, but the distinction clarifies why US is often grouped
with histogram-based estimators, while TI-type methods emphasize mean
forces.

:::{important}
:icon: false
### Free-energy guided enzyme design for plastic biodegradation with Umbrella Sampling

A recent example of the traditional use of US is the study of the
catalitic mechanism of PETase by Jerves et al.[@jerves2021reaction].
Polyethylene terephthalate (PET) is one of the most widely used
plastics, and the discovery of Ideonella sakaiensis PETase suggested a
biological solution to plastic recycling, but its catalytic mechanism is
difficult to understand. provided the first quantitative free-energy
description of PETase catalysis, mapping the full reaction cycle with
QM/MM umbrella sampling. Using carefully chosen bond-breaking/formation
reaction coordinates, they reconstructed the free-energy landscape of
the acylation and deacylation steps. Despite the absence of more modern
collective-variable discovery tools, the study showed that chemical
insight can still guide the selection of effective coordinates: the
resulting profiles shown in the figure below reproduced experimental
barriers (20.0 kcal/mol vs. 18--19 kcal/mol measured) and revealed a
concerted, tetrahedral transition state mechanism distinct from earlier
proposals. ![image](images/jerves.jpeg){width="0.65\\linewidth"}
*Free-energy profile of the acylation step with snapshots of the
reactant, tetrahedral transition state, and leaving group, highlighting
the mechanistic insight and agreement with experiment*

:::

Over the years, umbrella sampling has inspired numerous extensions.
Adaptive umbrella sampling [@mezei1987adaptive]iteratively builds the
bias toward uniform sampling; local elevation umbrella sampling
[@huber1994local; @hansen2010using] introduces a history-dependent bias
akin to early metadynamics [@laio2002escaping], of which it can be
considered a precursor. Multidimensional formulations like that of
Bartels and Karplus [@bartels1997multidimensional] or Kästner
[@kastner2009umbrella]further allow simultaneous treatment of coupled
coordinates. These methods have been hybridized with other techniques,
leading to efficient self-learning adaptive variants [@wojtas2013self]
or, as in the case of the combination of US with replica exchange
techniques, versions that are amenable to extreme parallelization on
supercomputers [@jiang2012calculation], showing that US is more than
ever a relevant algorithm in free energy landscape calculations.

## Constrained reaction coordinates: the Blue Moon Ensemble

Instead of using a Harmonic restraint as in US, one can use instead
holonomic constraints [@goldstein1950classical; @van1984constraints] to
enforce a specific value of the CV $\xi(\mathbf {r})=s$ using algorithms
like SHAKE [@ryckaert1977numerical], where the constraining force acts
along $\partial\xi/\partial \mathbf{r}$. In simple cases, the update of
some atomic coordinates can be prevented.

When computing the PMF from a dynamic with such a constraint, two
geometric corrections appear in the conditional identity in terms of a
coordinate-only average, namely an unbiasing factor
$Z_{\xi}= \sum_i \frac{1}{m_i}\left|\partial {\xi}/\partial \mathbf r_i\right|^2$
that corrects for the loss of momentum along the constraint. The
connection between conditional averages and (biased) averages in the
presence of a constraint is [@carter1989constrained]
$$\left\langle X \right\rangle_{\xi=s}^\mathrm{cond} = \frac{\left\langle \sqrt{1/Z_\xi}  X \right\rangle_{\xi=s}}{\left\langle \sqrt{1/Z_\xi}  \right\rangle}_{\xi=s}$$
and the full mass-metric tensor that appears when integrating over (all,
including the unbiased) kinetic degrees of freedom. The full Blue Moon
ensemble configurational formula (so named because it helps sample
events that happen "once in a blue moon") is $$\frac{dW}{ds}
=\frac{
\displaystyle \left\langle \sqrt{1/Z_\xi}\left[ \frac{\partial V}{\partial s}-\frac{kT}{2}\frac{\partial \ln|M(q)|}{\partial s}\right]\right\rangle_{\xi=s}}
{\displaystyle \left\langle \sqrt{1/Z_\xi}\right\rangle_{\xi=s}}.$$ This
formulation might be complicated to evaluate, in particular because the
RC needs to be one of the generalized coordinates used to describe the
configurations, and Sprick and Ciccotti, in a work that presents the
whole Blue Moon ensemble in a very effective way [@sprik1998free],
derived an equivalent one that requires only the constraint force
magnitude $F_\xi$, $$\frac{dW}{ds}
=\frac{\displaystyle \left\langle \sqrt{1/Z_\xi}\left[ -F_\xi+kT G_\xi \right]\right \rangle_{\xi=s}}{\displaystyle \left\langle \sqrt{1/Z_\xi}\right\rangle},$$
along with a curvature correction
$$G_\xi=\frac{1}{Z_\xi^{2}}{\sum_{i,j}\frac{1}{m_i m_j}
\frac{\partial \xi}{\partial \mathbf r_i}\frac{\partial^2 \xi}{\partial \mathbf r_i\partial \mathbf r_j}\frac{\partial \xi}{\partial \mathbf r_j}}$$
In many common cases like that of a simple distance,
$\xi=r_{ij}$,$Z_\xi$ is constant and $G=0$, so the weights cancel out.
The case of multidimensional reaction constants is discussed in
Ref.[@ciccotti1991molecular].

:::{important}
:icon: false
# The trimer paradox and the geometric bias

[ ]{style="background-color: white"}

For a trimer of fixed bond lengths, one might naively expect the
internal angle $0\le\chi\le\pi$ to be uniformly distributed over the
sphere of bond orientations, giving the density
$P(\chi) = \frac{1}{2}\sin \chi$ as in a random-walk picture. However,
Kramers' calculation [@kramers1946behavior] showed that the correct
distribution carries an extra factor from the phase-space measure (the
determinant of the metric tensor),
$$P(\chi) \propto \sin\chi\,\sqrt{1 - \frac{1}{4} \cos^2 \chi},$$ which
favors right-angled conformations. This "bias" arises because the
moments of inertia, and hence the accessible momentum-space volume,
depend on $\chi$. If one replaces the rigid rods with stiff harmonic
springs ("Fraenkel springs") and then takes the infinite-stiffness
limit, the angle distribution reverts to the naive result
$P(\chi) \propto \sin\chi$, i.e., uniform on the sphere. So, an
infinitely stiff bond is not the same as a rigid one! The resolution of
this paradox [@van1984constraints] is that imposing holonomic
constraints induces an entropic correction of geometric origin in the
effective free energy,
$$A_{\mathrm{geom}}(\chi) = -kT \ln \sqrt{\det M(\chi)},$$ with
$M(\chi)$ the mass-metric tensor. Suppose one wants to "unbias"
constrained simulations back to the uniform-sphere distribution. In that
case, this geometric term must be explicitly removed, and this term
usually takes the name of Fixman potential [@fixman_classical_1974].

:::

It's essential to note that the approach to treating a constraint
depends on the reason it was introduced. For example, the constraints
used in the Blue Moon are an artifact of the method, and their biasing
effect must be removed. The bias of constraints that keep molecular
structures rigid (typically, bonds or angles in empirical force field
simulations) can be removed, at least in some cases, via the Fixman
potential [@fixman1978simulation]. Whether removing this bias is the
correct thing to do or not is an open point, as, in general, both
flexible and constrained bonds are approximations. The former do not
take into account that excited vibrational states are often way above
the thermal energy. In contrast, the second disregards that the
zero-point energy of the system, and thus the bond lengths, depend on
the molecular conformation [@van1984constraints]. Even if not strictly
rigid, however, these degrees of freedom do not obey the equipartition
theorem [@tolman1918general] and thus unbiasing them is unlikely to be
the correct approach.

## Adaptive Bias Methods: Metadynamics

Metadynamics (MetaD), introduced by Laio and Parrinello
[@laio2002escaping], is one of the most influential adaptive-bias
algorithms for reconstructing FESs
[@metaD_2; @metaD_1; @bussi2020using; @valsson2016enhancing]. Its
central idea is to discourage a molecular simulation from revisiting
previously explored regions of the space of CVs $\xi$; in doing so,
metadynamics enhances the fluctuations along $\xi$, thus speeding up the
sampling [@valsson2016enhancing]. A MetaD simulation evolves under a
time-dependent bias potential $V(\xi,t)$ that is incrementally
constructed as the trajectory progresses. At regular time intervals, a
small repulsive Gaussian hill of height $w$ and width $\sigma$ is
deposited at the instantaneous CV value $\xi(t)$:
$$V(\xi,t) = \sum_{t'<t} w
e^{-\frac{\left(\xi-\xi(t')\right)^2}{2\sigma^2 }}$$ This "computational
sand-filling" [@metaD_2] progressively raises the free-energy of
ensembles of configurations visited during sampling, allowing the system
to escape local minima and visit new regions of phase space. In the
long-time limit, the accumulated bias offsets the underlying free-energy
surface $F(\xi)$ up to an additive constant, so that
$V(\xi,t \to \infty)\approx -F(\xi)$. Once this condition is reached,
the biased dynamics samples a uniform probability distribution in CV
space, effectively restoring ergodicity (see Sec.
[sec:Theory](#sec:Theory)).

MetaD is conceptually related to other history-dependent approaches,
such as the local elevation method [@localelevation1994]. In particular,
MetaD shares with local elevation the general principle of discouraging
revisits to previously explored regions of collective variable space,
while differing in its formulation and bias-update protocol. Moreover,
compared with earlier mean-force-based schemes such as the Adaptive
Biasing Force (ABF) method [@abf_2015; @abf_darve2001; @abf_darve2002],
metadynamics constructs the bias from local visitation history rather
than explicit force estimates.

Despite its simplicity and general applicability, MetaD suffers from a
few drawbacks: the continual deposition of Gaussians can lead to
systematic overshooting of the free-energy surface; convergence depends
on the choice of Gaussian height and width; and the time dependence of
$V(\xi,t)$ complicates rigorous reweighting. These issues motivated the
development of statistically controlled variants that address these
shortcomings [@metaD_2; @valsson2016enhancing; @bussi2020using].

:::{important}
:icon: false
### Free-energy landscape of CO reduction on copper[]{#sec:case-study-reduction label="sec:case-study-reduction"}

An excellent example of the use of metadynamics to discover reaction
landscapes is the work by Cheng, Xiao, and Goddard III[@cheng2017full]
on the mechanism of CO reduction on copper. Copper remains the only
elemental catalyst capable of reducing CO$_2$ into hydrocarbons at
significant rates, but its product distribution and mechanistic pathways
have long been debated. Using ab initio molecular dynamics with explicit
water layers, combined with metadynamics and refined using the Blue Moon
ensemble, the authors computed atomistic free-energy barriers and
pathways. The analysis revealed that at moderate potentials (U \> --0.6
V vs RHE, pH 7), ethylene is the dominant product, formed via CO--CO
coupling in an Eley--Rideal pathway with water as the proton source
($\Delta G^\ddag = 0.69$ eV). At more negative potentials, hydrogen
competes for surface sites, suppressing C--C coupling and enabling
methane formation, with $^*$CHO identified as the key intermediate.

The impact of this study is twofold. First, it demonstrated how explicit
solvation and constant-potential modeling resolve longstanding
discrepancies in previous DFT work, where implicit solvation gave
inconsistent barriers. Second, by quantitatively reproducing the
observed potential- and pH-dependent product distribution, it suggested
a mechanism for for tuning selectivity in electrochemical CO$_2$
utilization, influencing subsequent efforts to design Cu-based alloys.

![image](images/goddard.pdf){width="\\linewidth"} *Free-energy landscape
for ethylene formation on Cu(100). (a) snapshots from ab-initio
simulations with explicit solvent revealed (b) that the Eley--Rideal
mechanism (black) has consistently lower barriers than
Langmuir--Hinshelwood (blue), establishing CO dimerization as the
rate-determining step.*

:::

### Well-Tempered Metadynamics (WTMetaD)

Well-tempered metadynamics [@wt_metaD_1] introduces a smooth tempering
of the bias deposition rate to achieve self-limiting convergence. In
WTMetaD the Gaussian height decreases exponentially with the local value
of the bias already accumulated:
$$w(t) = w_0 e^\frac{-\beta{V(\xi(t),t)}}{(\gamma-1)}$$

where $\gamma = (T + \Delta T)/T > 1$ is the *bias factor*. At the
beginning of a simulation, Gaussians start with height $w_0$; as
$V(\xi,t)$ grows, the added bias diminishes, so that (V) asymptotically
approaches a fraction of the underlying free energy:
$$V(\xi,t\to\infty) = -({\gamma-1})^{-1} F(\xi).$$

The bias factor $\gamma$ tunes the trade-off between exploration (large
$\gamma$) and accuracy (small $\gamma$). The stationary distribution
sampled by WTMetaD in the long-time limit is no longer flat but
*well-tempered*, i.e. $p(\xi)\propto {e^{-\beta F(\xi)/\gamma}}$. The
sampling along $\xi$ therefore occurs at an effectively elevated
temperature $T_\text{eff}=\gamma T$, which thus enhances barrier
crossing while maintaining a known, analytically recoverable bias.
WTMetaD has, *de facto*, become the standard formulation implemented in
modern software (e.g., PLUMED [@plumed214], GROMACS [@Lindahl2022],
LAMMPS [@Thompson2022]) because it provides controlled convergence,
improved statistical efficiency, and the possibility to monitor the
flattening of the free-energy landscape on-the-fly.

One can combine WTMetaD with static biases (e.g., harmonic restraints,
walls, or custom static biases) in a straightforward, additive manner
[@Awasthi2016; @limongelli2013funnel; @bjola2024estimating]. If the
WTMEtaD bias is deposited sufficiently slowly - so that transition
states remain effectively bias-free - the infrequent metadynamics
framework allows barrier-crossing times to be rescaled to recover
physical rate constants
[@tiwary2013metadynamics; @salvalaglio2014assessing; @palacio2022transition].

:::{important}
:icon: false
### Choosing the right coordinate: the ring puckering example

![image](Figures/Figure_puckering_2.png){width="1\\linewidth"} The
choice of collective variables (CVs) is critical in free-energy
calculations, but not always obvious. Puckered ring conformers can be
described by the Cremer--Pople cartesian coordinates, obtained from the
out-of-plane displacements $z_j$ of the 6-membered ring atoms as\
$$q_x =  \sqrt{\frac{1}{3}} \sum_{j=1}^6 z_j \cos\left[\frac{2\pi}{3}(j-1)\right],\quad  q_y = -\sqrt{\frac{1}{3}} \sum_{j=1}^6 z_j \sin\left[\frac{2\pi}{3}(j-1)\right],\quad q_z = \sqrt{\frac{1}{6}} \sum_{j=1}^6 (-1)^{j-1} z_j,$$
or by their polar representation
$\left(Q \sin \theta \cos \phi, Q \sin \theta \sin \phi, Q\cos \theta\right)$.
These coordinates correspond to a discrete Fourier decomposition of the
atomic elevations over the mean molecular plane, with the angular
coordinates spanning all pseudorotations (middle panel, where $\theta=0$
and $\pi$ correspond to the chair and inverted chair conformations,
respectively). Only the coordinates $(\theta,\phi)$ turn out to be
useful biasing variables, as they control connectivity between
conformers. The left panel shows the fully sampled puckering free energy
landscape of glucuronic acid from a metadynamics run using
$(\theta,\phi)$ as CVs (5 kJ/mol isolines). In contrast, biasing along
the Cartesian projection $(Q\sin\theta\cos\phi,\, Q\sin\theta\sin\phi)$
might seem to work well initially, but only up to the equatorial line,
where boats and twisted boats conformers are located. There, the bias
force is perpendicular to the puckering sphere surface and only promotes
ring expansion/contraction, breaking the ergodic sampling. The right
panel shows the histogram of $Q$ and $q_r=|(q_x,q_y)|$ sampled during a
metadynamics run that uses $(q_x,q_y)$ as CVs. The algorithm becomes
stuck stretching the ring, as indicated by the strong correlation
between $q_r$ and the ring deformation $Q$, and is unable to leave the
northern (chair-like) hemisphere. Using the Cartesian projections as
CVs, the free energy estimates of accessible conformers are heavily
biased [@sega2009calculation].

:::

## Free-Energy Estimators for Metadynamics and Well-Tempered Metadynamics

The bias potential accumulated during a MetaD or WTMetaD simulation
modifies the underlying probability distribution, so that direct
estimates of the free energy $F(\xi)$ from the bias $V(\xi,t)$ are
inherently time-dependent. Several formulations have been proposed to
obtain *time-independent free-energy estimators*, which recover $F(\xi)$
or the distribution of *other* observables from a trajectory sampled
under the effect of a time-dependent bias
[@metaD_reweight; @wt_metaD_2; @Marinova2019; @giberti2019iterative; @Ono2020MetaD].

### Tiwary--Parrinello Time-Independent Estimator

Tiwary and Parrinello [@metaD_reweight] address the metadynamics
limitation that the evolving bias $V(\xi,t)$ yields $F(\xi)$ only up to
a time-dependent constant. Their approach is based on the observation
that, in the quasi-stationary regime, the instantaneous distribution is
written as
$$p(\mathbf{r},t)=p_0(\mathbf{r})\,e^{-\beta\,[V(\xi(\mathbf{r}),t)-c(t)]}
\label{eq:metad_sampled_prob}$$ where $p_0(\mathbf{r})$ is the Boltzmann
distribution, and the offset $c(t)$ is defined by:
$$c(t)=\beta^{-1}\ln\frac{\displaystyle\int d\xi\,e^{-\beta F(\xi)}}{\displaystyle\int d\xi\,e^{-\beta F(\xi)+\beta V(\xi,t)}}
\label{eq:coft}$$ This makes explicit that $c(t)$ is the additive
correction that restores the unbiased Boltzmann measure.

Introducing a scaled time $\tau$ via $d\tau/dt=e^{\beta c(t)}$, they
derive a \*\*time-independent free-energy estimator\*\* valid after a
short transient and for both well-tempered (WT) and standard
metadynamics:
$$F(s)= -\frac{\gamma}{\Delta T}\,k_BT\,V(\xi,t)+k_BT\ln\int d\xi\,\exp\left[\frac{\gamma}{\Delta T}\,\frac{V(\xi,t)}{k_BT}\right],$$
with $\gamma=(T+\Delta T)/T)$. This expression removes the explicit time
dependence of $F(s)$, enabling local convergence checks and direct
comparison of FESs from simulations performed with different parameters.
Moreover, Ref. [@metaD_reweight] provides a practical route to compute
$c(t)$ on the fly.

Once $c(t)$ is available, reweighting generic observables from biased
trajectories follows from:
$$\langle O\rangle_0=\big\langle O(\mathbf R)\,e^{\beta\,[V(\xi(\mathbf R)\,t)-c(t)]}\big\rangle_b$$
which is analogous to the Zwanzig reweighting typically applied to
time-independent biases [@Zwanzig1954] (see Sec. [sec:FEP](#sec:FEP)).
This approach yields a rigorous reweighting for any observable, under
the assumption that the bias evolves slowly compared to CV relaxation.
It is especially effective in the well-tempered limit, where the bias
growth rate diminishes exponentially with time
[@marinova2019time; @gimondi2018building].

### Bonomi--Barducci--Parrinello Reweighting.

Bonomi et al. introduce a simple, general reweighting scheme for WTMetaD
that recovers unbiased Boltzmann statistics of any observable---starting
from the same key identity defining $P(\mathbf r,t)$ (Eq.
[eq:metad_sampled_prob](#eq:metad_sampled_prob)), the definition of
$c(t)$ (Eq. [eq:coft](#eq:coft)) and the long-time limit of the bias
constructed with WTmetaD:
$V(\xi,t\to\infty)=-\Delta T/(\Delta T+T)\,F(\xi)$, Bonomi et al.
develop a reweighting approach that circumvents the need of computing
$c(t)$. By differentiating Eq.
[eq:metad_sampled_prob](#eq:metad_sampled_prob) for small time intervals
$\Delta t$, an evolution equation that eliminates $c(t)$ is derived:
$$p(\mathbf r,t+\Delta t)=e^{-\beta\,[\dot V(\xi(\mathbf r),t)-\langle \dot V(\xi,t)\rangle]\Delta t}\,p(\mathbf r,t)
\label{eq:propagator}$$ where $\dot c(t)=-\langle \dot V(\xi,t)\rangle$
and the average is over the biased distribution at time $t$. For WTMetaD
with Gaussian depositions, these results lead to a practical reweighting
algorithm based on three steps. (i) Accumulate a joint histogram
$N_t(\xi,f)$ for a target variable $f(\mathbf r)$ between Gaussian
updates; (ii) at each update, compute $\dot V$ and $\dot c$ using the
current accumulated histogram, and evolve $N_t$ using Eq.
[eq:propagator](#eq:propagator); (iii) reconstruct the unbiased
distribution of $f(\mathbf{r})$ as
$$p(f)=\frac{\sum_{\xi} e^{+\beta V(\xi,t)}\,N_t(\xi,f)}{\sum_{\xi,f} e^{+\beta V(\xi,t)}\,N_t(\xi,f)}.$$
The method is lightweight as it does not require any a posteriori
calculation of the total energy, it works in post-processing or (in
principle) on-the-fly, and it converges efficiently as shown in Refs.
[@gimondi2018building; @Marinova2019].

#### Mean Force Integration (MFI)

Extending Umbrella Integration (UI, see section
[sec:UmbrellaSampling](#sec:UmbrellaSampling)) to time-dependent biases,
Mean Force Integration (MFI) provides a general estimator for
history-dependent biasing schemes
[@marinova2019time; @bjola2024estimating]. Rather than computing the
non-local, time-dependent bias average $c(t)$, MFI reconstructs the FES
by integrating the *mean force* in $\xi$, computed at each bias-update
step: $$\nabla F_t(s) = -\beta^{-1}\nabla\ln p_b^t(s) - \nabla V_t(s),$$
where $p_b^t(s)$ is the biased probability density sampled while the
bias $V_t(s)$ remains unchanged between updates. Averaging these
mean-force estimates over successive updates and integrating numerically
yields a *time-independent FES*. This formulation reveals that
metadynamics, despite its adaptive nature, can be rigorously interpreted
within the TI framework: the accumulated bias corresponds to an
integrated mean force along the collective variables, and as such, it
circumvents the need to formulate equilibration assumptions on the bias
evolution. It can be applied to standard, well-tempered,
adaptive-Gaussian, or transition-tempered MetaD variants. Importantly,
MFI naturally supports *ensemble aggregation*---it can merge sampling
from multiple independent metadynamics runs without requiring continuous
trajectories or recrossings
[@marinova2019time; @bjola2024estimating; @serse2024unveiling].

### Variationally Enhanced Sampling (VES)

In contrast to the history-dependent or kernel-based approaches of
metadynamics, VES [@ves_valsson2014] formulates the problem of finding
the bias potential as a variational minimization of a functional of the
bias potential $V(\xi)$. Specifically, the stationary condition of the
functional ensures that the biased ensemble reproduces a desired
*target* probability distribution $p^*(\xi)$.

The bias potential is defined as the function $V(\xi)$ that minimizes
the Kullback--Leibler (KL) divergence between the sampled distribution
$p_V(\xi)$ and the target distribution:

$$\Omega[V] = \frac{1}{\beta} \ln \left[\int d\xi e^{-\beta [F(\xi) + V(\xi)]} \right]
 \int d\xi p^*(\xi) V(\xi),$$

where $F(\xi)$ is the underlying free energy. Minimizing $\Omega[V]$
with respect to $V$ yields the optimal bias

$$V^*(\xi) = -F(\xi) - \frac{1}{\beta} \ln p^*(\xi) + \text{const.}$$

In practice, the bias is expressed as a linear combination of basis
functions (e.g., polynomials, splines, or neural-network features) with
parameters optimized during the simulation through stochastic gradient
descent. This variational approach offers a systematic method for
constructing bias potentials that provide direct control over the
sampled distribution. When $p^*(\xi)$ is chosen to be uniform, VES
converges to a direct estimate of the free energy $F(\xi)$; when it is a
tempered distribution, it behaves analogously to well-tempered
metadynamics but with improved smoothness and convergence properties. A
key strength of VES is how naturally it fuses with modern ML: starting
from VES variational functional [@ves_valsson2014], the bias can be
parameterized by a neural network and optimized directly from simulation
--- an approach realized by Bonati et al. [@bonati2019neural]
("Deep-VES"), which treats the VES objective $\Omega[V]$ as a
differentiable loss and updates network parameters using gradients
estimated from the biased and target ensembles.

### On-the-fly Probability Enhanced Sampling (OPES)

OPES [@invernizzi2020rethinking; @invernizzi2020unified] extends
metadynamics by adopting a direct probabilistic formulation. Instead of
depositing Gaussians, OPES continuously estimates the marginal
probability distribution of the CVs and updates a bias potential
designed to transform the instantaneous distribution into a chosen
*target distribution* $\tilde p(\xi)$. In practice, the target is often
chosen to be uniform (for direct free-energy estimation) or follows a
well-tempered form to strike a balance between exploration and
stability. At every step, OPES computes the current biased histogram
$p_V(\xi)$ and defines the new bias as
$$V(\xi) = -k_BT \ln\left[\frac{p_V(\xi)}{\tilde p(\xi)}\right].$$ This
ensures that, upon convergence, the simulation samples
$p_V(\xi)=\tilde p(\xi)$. The bias, therefore, evolves self-consistently
to realise a desired stationary distribution rather than through
incremental hill deposition. In the *OPES-Explore* variant,
$\tilde p(\xi)$ is flat, providing a direct reconstruction of the FES;
in *OPES-Meta* the target adopts the same tempered form as WTMetaD,
producing controlled exploration similar to well-tempered sampling but
with faster convergence and reduced noise. Because OPES derives from an
explicit reweighting equation, it inherits a clear statistical
interpretation, and the accumulated bias approximates the free energy
according to $F(\xi) = -(\gamma-1) V(\xi) + \text{const}$, analogous to
WTMetaD but without relying on discrete Gaussian hills. The absence of
kernel summations makes OPES computationally cheaper and smoother in
high-dimensional CV spaces. Moreover, its probabilistic update scheme
naturally accommodates on-the-fly reweighting and can exploit adaptive
kernel density estimators to achieve rapid convergence even in
multi-dimensional landscapes. Finally, OPES offers a direct route to
target distributions using CVs that can be learned, enabling an
aggressive and efficient yet controlled exploration with a sound
statistical reweighting procedure
[@henin2022review; @invernizzi2020unified].

## Free Energy Profiles and Equilibrium Constants

Through the link between probabilities and partition functions, free
energy profiles and the associated PMFs can be used to compute
equilibrium constants for binding reactions. One must be careful when
connecting to standard free energies of reaction, since the equilibrium
constants defined in statistical mechanics depend on the reference
concentration. Experimental $\Delta G^\circ$ values are typically
referring to the standard concentration of $c^\circ=1$ M.

For an association reaction of the kind $R+L \rightleftharpoons RL$ the
law of mass action at low concentrations gives
$$K_{\rm eq}=\frac{[RL]}{[R][L]},\qquad
\Delta G_{\rm bind}^\circ=-k_BT\ln\left(K_{\rm eq}c^\circ\right).$$ Here
we use $\Delta G_{\rm bind}^\circ$ to denote the standard free energy of
binding, following the convention in alchemical binding studies. The law
of mass action formally defines the binding equilibrium constant above.
Still, this definition is valid in the thermodynamic limit only, and
cannot be applied directly in molecular simulations, which typically
contain a single receptor and ligand in a finite
box[@de2011determining]. A statistical--mechanical route to express the
equilibrium constant that is more appropriate for small systems is using
the ratio of Boltzmann probabilities for bound versus unbound
configurations,
$$\Delta G^0_{\mathrm{bind}} \approx \Delta A^0_{\mathrm{bind}} = -kT \ln \frac{P(RL)}{P(R+L)} - kT\ln \left(c^0/c\right),\label{eq:deltaG_shirts}$$
where the effect of (typically small) volume changes has been neglected,
and the term $c^0/c$ converts from the molar concentration in the
simulation box, $c$, to the standard state.

If $\Omega_{\rm site}$ denotes the binding region and
$\Omega_{\rm bulk}$ the unbound region, then
$$\frac{P(RL)}{P(R+L)}  = \frac{\int_{\Omega_{\rm site}} e^{-\beta U(\mathbf q)} d\mathbf q }{\int_{\Omega_{\rm bulk}} e^{-\beta U(\mathbf q)} d\mathbf q} = \frac{Q_{RL}}{Q_{R+L}}$$
and the (dimensional) equilibrium constant is then written in terms of
probabilities and the simulation box volume as
$$K_{\rm eq}=V\frac{Q_{RL}}{Q_{R+L}}\label{eq:k-shirts}$$ A similar,
equivalent, form is used by Roux, who writes: $$K_{\rm eq}=
\frac{\int_{\Omega_{\rm site}} e^{-\beta U(\mathbf q)} d\mathbf q }{\int_{\Omega_{\rm bulk}} \delta(\mathbf     q - \mathbf q^*) e^{-\beta U(\mathbf q)} d\mathbf q},$$
where the delta, which pins the ligand in the bulk, would yield the $V$
factor in Eq.[eq:k-shirts](#eq:k-shirts) once integrated if isotropy and
homogeneity can be assumed in the bulk. As binding affinity simulations
are typically performed using a series of restraints, their effect has
to be carefully removed by unbiasing the
results[@woo2005calculation; @roux2008comment].

Gilson and coworkers derived the same constant directly from activities
for a rigid ligand
$$\Delta G^\circ_\mathrm{bind} = -RT \ln\left( \frac{c^\circ}{8\pi^2}\frac{\sigma_R \sigma_L}{\sigma_{RL}} \frac{Q_{RL},Q_S}{Q_{R},Z_{R}} \right)+ P^\circ \Delta \bar V_{AB}.$$
where the factor $8 \pi^2$ comes from the integration of momenta
(constraints are not unbiased in this picture), the symmetry numbers
$\sigma_i$ take into account the degeneracy of configurations, and $Q_S$
is the configurational partition function of the solvent. Here, the
contribution coming from volume changes
$\Delta \bar V_{LR} = V_{LR} -V_L -V_R$ is spelled out explicitly. The
connection with Eq.[eq:deltaG_shirts](#eq:deltaG_shirts) comes from the
infinite dilution identity $Q_RQ_L/ Q_S = Q_{R+L}$.

This formulation serves as the basis for the double--decoupling method
(DDM), in which the ratio of partition functions is obtained by
decoupling the ligand in the binding site and in the bulk solvent.
Because simulations in the bound state require restraining the ligand's
position and orientation relative to the receptor, one must add back the
analytical free energy cost of these restraints. Boresch and co-workers
introduced a minimal and non-redundant set of six relative restraints
(one distance $r$, two bond angles $\theta_i$, and three torsional
$\phi_i$, for which the correction has a closed form,
$$\Delta G_{\rm restr} = -k T \ln \left[\frac{8\pi^2 V \sqrt{K_r K_{\theta_1} K_{\theta_2} K_{\phi_1} K_{\phi_2} K_{\phi_3}}}{r_0^2 \sin\theta_{1,0}\sin\theta_{2,0} , (2\pi kT)^3} \right],$$
where $r_0, \theta_{i,0}$ are the equilibrium restraint values and $K_i$
are the force constants. The standard binding free energy has to be
corrected by a similar term for reference values[@boresch2003absolute].

Various formulations of the DDM method rest on the same principle: the
binding free energy is the difference between decoupling the ligand in
bulk and in the binding site. In this way, DDM should be viewed less as
a single protocol than as a general framework, encompassing related
alchemical cycles such as confine-and-release. The central lesson is
that restraints and their proper unbiasing are essential to obtain
rigorous and transferable results, which is why DDM remains the standard
route to absolute binding free
energies.[@gilson2007calculation; @bian2025formally]

## Reaction Paths on the Free Energy Landscapes and Reaction Kinetics

### String-based and minimum-action path optimization

While FESs capture the thermodynamic stability of molecular states, they
also provide the natural framework for investigating reaction paths and
associated kinetics. Before the emergence of trajectory-based sampling
approaches such as Transition Path Sampling, a family of algorithms had
been developed to compute representative transition pathways by
minimizing an effective energy or action functional, the nudged elastic
band (NEB) method, introduced by Jónsson and co-workers
[@jonsson1998nudged] and later refined [@henkelman2000improved],
determines minimum-energy paths on potential-energy surfaces by relaxing
a discretized chain of replicas connecting two metastable basins until
the perpendicular component of the force vanishes. NEB is a remarkably
effective algorithm that is still one of the workhorses in the
exploration of reactive pathways. Extensions and adaptations of these
ideas include, for example, the "nebterpolation" method
[@wang2016automated], which automates the identification and refinement
of reaction paths from molecular-dynamics trajectories, the generalized
solid-state nudged elastic band, which allows studying reaction pathways
of solid--solid transformations, and the recent NEB-TS for improved
convergence in the search of reactive paths [@asgeirsson2021nudged]. In
contrast to trajectory-sampling approaches, such as Transition Path
Sampling, which define the transition state probabilistically through
ensembles of reactive trajectories, NEB-type methods locate it
energetically as the saddle point along a typically minimum-energy path.

Extending the NEB concept to finite temperatures and taking into account
entropic effects, Ren and Vanden-Eijnden developed the string (or tube)
method, in which a continuous curve evolves under the projected mean
force until it converges to the minimum free-energy path connecting the
metastable regions
[@weinan2002string; @weinan2005finite; @ren2007simplified]. These
geometric approaches offer an intuitive representation of transitions on
complex free-energy surfaces, inspiring numerous subsequent algorithms
for exploring reactive pathways.

Another similar class of approaches is based on the Onsager-Machlup
action [@onsager1953fluctuations; @adib2008stochastic], which, when
extremized, provides the most probable path connecting two states in the
canonical ensemble, assuming that the overdamped Langevin equation
governs the underlying dynamics. Building on these variational
principles and switching from a time-dependent formulation to an energy
one via the Hamilton-Jacobi formulation of mechanics, Faccioli and
coworkers formulated the Dominant Reaction Pathway (DRP) method
[@elber2000temperature; @faccioli2006dominant; @sega2007quantitative; @autieri2009dominant].
The action to be minimized takes the form.
$$S_{HJ}(\mathbf{x}_0,\mathbf{x}_1) = \int_{s_0}^{s_1} ds \sqrt{2E_\mathrm{eff} + D^2 \left( \frac{1}{kT} \frac{\partial U(\mathbf{x}(s))}{\partial \mathbf x} \right)^2 
- \frac{2D^2}{kT} \frac{\partial^2 U(\mathbf{x}(s))}{\partial \mathbf x^2} },$$
Where $D$ is the diffusion coefficient, $E_\mathrm{eff}$ selects the
transition time, and the Laplacian term modulates the local entropic
contribution. This method has recently been recast in a form suitable
for quantum computing, by mapping the search for dominant paths onto an
Ising optimization problem solvable via quantum annealing
[@hauke2021dominant].

In systems characterized by rare transitions between metastable basins,
dynamics can be viewed as sequences of transitions through the narrow
regions of configuration space that connect these basins. Quantifying
these transition events requires linking the static information encoded
in $F(\xi)$ to the probability and rate of barrier crossings. This
connection underlies transition-state theory (TST) and its descendants,
which are based on the transition path ensemble.

### The Bennett-Chandler algorithm

In equilibrium systems, the rate constant for transitions from a basin
$A$ to another $B$ can be expressed as a time correlation function,
known as the Bennett--Chandler (BC) relation:
$$k_{AB} = \frac{\langle \dot{\xi}(0)\, \delta[\xi(0)-\xi^\ddagger]\, h_B[\xi(t)]\rangle}
{\langle h_A \rangle},$$ where $\xi(x)$ is a reaction coordinate,
$\dot\xi$ its time derivative, $\xi^\ddagger$ defines a dividing
surface, and $h_A$, and $h_B$ are characteristic functions identifying
configurations within $A$ and $B$. In the short--time limit $t\to{}0^+$,
where no recrossings occur, this reduces to the transition--state theory
(TST) rate: $$k_\mathrm{TST} =
\frac{\langle \dot{\xi}(0)\,\theta[\dot{\xi}(0)]\,\delta[\xi(0)-\xi^\ddagger]\rangle}
{\langle h_A\rangle}.$$ The ratio $\kappa = k_{AB} / k_\mathrm{TST}$
defines the transmission coefficient discussed in
Sec.[sec:kinetics](#sec:kinetics), which quantifies dynamical
recrossings of the dividing surface and measures the deviation from
ideal TST behavior.

Because both expressions depend on averages over configurations near the
transition state $(\xi^\ddagger$), efficient evaluation typically
requires enhanced sampling. This can be achieved, for example, with US
along the coordinate $\xi$, or equivalently by constraining
$\xi=\xi^\ddagger$ and computing averages in the Blue Moon ensemble.
Both techniques concentrate sampling around the dividing surface where
the reactive flux originates.

When barrier crossings are highly diffusive, frequent recrossings make
$\kappa \ll 1$, but the relative statistical error scales as
$\Delta \kappa / \kappa \sim  1/ (\kappa\sqrt{n}),$ With the number of
trajectories $n$, the number of simulations required for convergence
becomes prohibitive. Some methods, such as the Ruiz-Montero-Frenkel-Brey
[@ruiz-montero_efficient_1997] approach, improve the convergence of the
Bennett-Chandler method, but are still limited by the necessity to know
the location of the transition state and by the assumption that only one
state is relevant for the rate calculation. This is not true for more
complex landscapes, as noted by Vanden-Eijnden
[@vanden2006transition; @vanden2010transition], where one must sample
the bundle of transition paths.

### Transition Path Sampling[]{#sec:tps label="sec:tps"}

For a Markovian dynamics, the probability density of a trajectory
$\{\mathbf{r}\}=(\mathbf{r}_0,\mathbf{r}_1,\dots,\mathbf{r}_t)$ is
$$P[\{\mathbf{r}\}] = \rho(\mathbf{r}_0)\prod_{i=0}^{t-1}p(\mathbf{r}_{i+1}|\mathbf{r}_i),\label{eq:path-prob}$$
where $\rho(\mathbf{r}_0)$ is the equilibrium distribution and $p$ the
conditional propagator [@bolhuis2002transition]. The probability that a
trajectory initiated in $A$ reaches $B$ at time $t$ is
$$C(t) = \frac{\langle h_A(\mathbf{r}_0),h_B(\mathbf{r}_t)\rangle}{\langle h_A(\mathbf{r}_0)\rangle}
= \frac{\mathcal{Z}_{AB}(t)}{\mathcal{Z}_A},$$ with
$$\mathcal{Z}_A = \int\mathcal{D}\{\mathbf{x}\}h_A(\mathbf{r}_0)P[\{\mathbf{r}\}]\qquad
\mathcal{Z}_{AB} = \int \mathcal{D}\{\mathbf{r}\} h_A(\mathbf{r}_0)P[\{\mathbf{r}\}]h_B(\mathbf{r}_t),$$
where $\mathcal{Z}_A$ is the probability of a trajectory to start from
$\mathbf{r}_0$, regardles of where it ends at time $t$, and
$\mathcal{Z}_{AB}$ that to start in $r_0$ and end in $r_t$. These
probabilities are expressed in terms of path integrals, effectively
functional integrals over the set of all possible paths
[@feynman1948space; @kleinert2009path; @seifert2012stochastic]. The rate
constant, for example, follows from the long-time derivative
[@bolhuis2002transition],
$$k_{AB} = \lim_{t\to\infty}\frac{d}{dt}\left(\frac{Z_{AB}(t)}{Z_A}\right),$$
and the problem of evaluating the rate $k_{AB}$ reduces to sampling the
path ensemble in the same conceptual way as equilibrium properties are
obtained from the Boltzmann distribution.

It can be shown that one can generate trajectories that sample the
biased probability of starting in region $A$ and ending in region $B$
without recrossing (the transition path ensemble) in a straightforward
manner. Starting from one (even roughly sampled) reactive path, new
trial trajectories $\{\mathbf{r}'\}$ are generated by small stochastic
modifications and integrated forward and backward in time, accepting or
rejecting the new path $\{\mathbf{r}'\}$ using a Metropolis scheme
[@metropolis1949monte; @kalos2009monte; @frenkel_understanding_2023]. In
the microcanonical ensemble or other generalized microcanonical
ensembles with extended Hamiltonians, the acceptance probability becomes
remarkably simple [@dellago2002transition] becomes remarkably simple:
$$\text{acc}(\{\mathbf{r}\}\to\{\mathbf{r}'\})
= h_A\left(\{\mathbf{r}
_0\})h_B(\{\mathbf{r}_t]\}\right),$$ and all reactive proposals are
accepted. This is, in essence, the basic Transition Path Sampling (TPS)
algorithm. Having access to trajectories in this ensemble does not, at
first sight, help in the calculation of quantities like the rate
$k_{AB}$, as such a quantity is computed with a normalization factor
over the path starting in $A$ and ending at any point. However, TPS
provides direct access to the transmission function
$\kappa(t) = k(t)/k_{tst}$ via
$\kappa(t) = \langle \dot h_B(t) \rangle_{AB} / \langle \dot h_B(0) \rangle_{AB}$,
and the rate constant can be accessed by supplementing the TPS
calculation with a US in the TPE [@dellago1999calculation]. Having
access to the TPE enables a straightforward computation of the committor
function $p_B(\mathbf r)$ (also known as splitting probability,
originally introduced by Onsager [@onsager1938initial], see the
associated box in the CVs section). The committor is defined as the
probability that a configuration $\mathbf r$, with randomized momenta
drawn from the equilibrium distribution, will reach state $B$ before
returning to $A$. Because algorithms like TPS naturally generate
reactive trajectories that cross this separatrix, one can easily
identify transition-state configurations by launching short trajectories
from frames along the generated paths and selecting those for which
$p_B\simeq{0.5}$.

Related path-sampling methods include Transition Interface Sampling
(TIS) [@van2003novel; @van2005elaborating], Forward Flux Sampling (FFS)
[@allen2009forward], and Markov State Models (MSMs)
[@prinz2011markov; @chodera2014markov]. TIS extends the TPS framework by
introducing a hierarchy of interfaces between $A$ and $B$, and sampling
conditional path ensembles connecting successive interfaces. The rate is
then obtained as the product of these conditional probabilities,
multiplied by the initial flux through the first interface. FFS, on the
other hand, employs forward-only stochastic propagation of trajectories
across interfaces and is particularly suited to non-equilibrium or
stochastic dynamics where time reversibility does not hold. Finally,
MSMs describe kinetics in terms of transitions between discrete
metastable states, reconstructing long-time dynamics from many short
unbiased trajectories; rates and committors follow from the
eigenstructure of the resulting transition matrix.

Several refinements have been proposed to improve upon the original TPS
and other path-related algorithms, such as Precision Shooting
[@grunwald2008precision], S-Shooting [@menzl2016s], Aimless Shooting
[@peters2006obtaining], or combining it with different sampling
techniques, including metadynamics [@borrero2016avoiding] and replica
exchange [@van2007reaction]. The reader is referred to a recent review
for a comprehensive discussion of these developments
[@bolhuis2021transition].

## FESs from experiments: single molecule force spectroscopy[]{#sec:single-molecule-spectroscopy label="sec:single-molecule-spectroscopy"}

The development of single-molecule force spectroscopy (SMFS) techniques,
such as atomic force microscopy (AFM), optical, or magnetic tweezers,
has opened up the possibility of directly probing molecular free-energy
landscapes experimentally. In these setups, a molecule or macromolecular
complex is tethered between a surface and a microscopic probe, which
applies a controlled force or displacement while recording the
corresponding extension and work performed on the system. The reaction
coordinate $\xi$ in this case is naturally associated with the molecular
extension $q$ along the pulling direction. The mechanical response of
the molecule encodes the underlying PMF $F(q)$, which can be
reconstructed from nonequilibrium work measurements using exact
statistical-mechanical relations.

A central theoretical foundation of this connection is Jarzynski's
equality, which relates nonequilibrium work to equilibrium free-energy
differences: $$e^{-\beta \Delta G(t)} =
\left\langle e^{-\beta W(t)} \right\rangle ,
\label{eq:Jarzynski}$$ where $W(t)$ is the external work performed along
the pulling trajectory, and $\langle \dots \rangle$ denotes an average
over many repetitions of the process. Remarkably, Eq.
[eq:Jarzynski](#eq:Jarzynski) holds even for transformations driven
arbitrarily far from equilibrium, providing a formal bridge between
dynamical experiments and equilibrium thermodynamics. Hummer and Szabo
[@hummer2005free] extended this result to the reconstruction of a full
free-energy profiles along the molecular extension coordinate (q),
obtaining $$e^{-\beta G_0(q)} =
\left\langle
\delta[q - q(x(t))]\, e^{-\beta [W(t) - V(q(t),t)]}
\right\rangle ,
\label{eq:HummerSzabo}$$ where $V(q,t)$ is the time-dependent external
potential applied during pulling. For instance, in optical-tweezer or
AFM experiments, a harmonic trap of stiffness $k_s$ is displaced at
constant velocity $v$, $V(q,t)=\tfrac12 k_s(q-vt)^2$.

Equation [eq:HummerSzabo](#eq:HummerSzabo) thus provides an operational
route to obtain the equilibrium free-energy profile $F_0(q)$ from a
collection of nonequilibrium pulling trajectories. If the pulling
protocol is adiabatic---meaning it is so slow that the system remains at
equilibrium---then the mechanical work (W) equals the reversible work
$\Delta F$. The trap potential is effectively infinitely stiff. In this
quasistatic limit, the molecular coordinate follows the minimum of the
trap potential. In practice, experiments are rarely perfectly
quasistatic, and Eq. [eq:HummerSzabo](#eq:HummerSzabo) must be applied
in its complete nonequilibrium form. The stochastic dispersion of
measured work values---arising from thermal fluctuations, instrumental
noise, and molecular heterogeneity---requires averaging over a large
number of trajectories or the use of maximum-likelihood estimators to
converge the exponential average in Eq.
[eq:HummerSzabo](#eq:HummerSzabo). Despite these challenges, pioneering
work by Liphardt et al. [@liphardt2002equilibrium] provided the first
quantitative experimental validation of Jarzynski's equality by
unfolding single RNA hairpins with optical tweezers, demonstrating that
nonequilibrium pulling data can reproduce equilibrium free-energy
differences with sub-$kT$ accuracy.

Modern SMFS now routinely maps multidimensional free-energy landscapes
of biomolecules, synthetic polymers, and supramolecular assemblies. The
measured work distributions can be directly compared to simulations
using steered molecular dynamics (SMD) and fast-growth thermodynamic
integration, where analogous pulling protocols are applied
computationally. Such combined experimental--computational analyses
enable the identification of metastable intermediates and hidden
barriers, providing a quantitative picture of molecular stability and
kinetics under mechanical stress.

In this sense, single-molecule pulling experiments extend the conceptual
framework of free-energy surface reconstruction beyond the realm of
simulations. By exploiting nonequilibrium work theorems, they offer a
direct, experimentally accessible analogue to biased-sampling approaches
such as umbrella sampling or metadynamics, allowing one to read and
interpret molecular free-energy surfaces from the controlled deformation
of single molecules.

# References
